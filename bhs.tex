\documentclass{article}
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{showlabels}
\usepackage{latexsym}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{siunitx}

\usepackage{color}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage[hyperref=true,
                      isbn=false,
                      style=numeric-comp,
                      giveninits=true,
                      maxbibnames=99]{biblatex}
\addbibresource{biblio.bib}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{booktabs}
\setlength{\textwidth}     {15.0cm} \setlength{\textheight}
{21.0cm} \setlength{\evensidemargin}{ 0.5cm}
\setlength{\oddsidemargin} { 0.5cm} \setlength{\topmargin}
{-0.5cm} \setlength{\baselineskip}  { 0.7cm}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\lambdau}{\underline{\lambda}}
\newcommand{\muu}{\underline{\mu}}
\newcommand{\subsetinf}{\displaystyle\mathop{\subset}_{\infty}}

\newcommand{\halmos}{\hfill $\;\;\;\Box$\\}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{hip}{Assumption}[section]
\newtheorem{defi}{Definition}[section]

\begin{document}

\title{Towards an efficient penalty method for convex quadratic programming \footnote{This work was supported by FAPESP 2013/05475-7, 2017/18308-2 and CNPq.}}
\author{
  L. F. Bueno \thanks{Institute of Science and Technology, Federal
    University of S\~ao Paulo, S\~ao Jos\'e dos Campos-SP,
    Brazil. E-mail: lfelipebueno@gmail.com}
  \and
  G. Haeser \thanks{Department of Applied Mathematics, University of S\~ao Paulo, S\~ao Paulo-SP, Brazil. E-mail: ghaeser@ime.usp.br}
    \and
    L.-R. Santos \thanks{Department of Mathematics, Federal University of Santa Catarina, Blumenau-SC, Brazil. E-mail: l.r.santos@ufsc.br}
}

\date{\today}

\maketitle

\begin{abstract}

Interior point methods have attracted most of the attention in the recent decades for solving large scale convex quadratic programming problems. In this paper we take a different route as we present a penalty method for convex quadratic programming based on recent augmented Lagrangian developments for nonlinear programming. The motivation of this approach is that Newton Method can be efficient for minimizing a piecewise quadratic function. Moreover, since penalty methods do not rely on proximity to the central path, some of the inherent difficulties in interior point methods can be avoided. Also, a good starting point can be easily exploited, which can be relevant for solving subproblems arising from sequential quadratic programming, in sensitivity analysis and in branch and bound techniques. Well definedness and finite convergence is proved. Numerical experiments on separable strictly convex quadratic problems formulated from the \textsc{Netlib} collection show that our method can be competitive with interior point methods, in particular when a good initial point is available.\\

\noindent {\bf Key words:} Linear programming, convex quadratic programming, augmented Lagrangian, penalty methods, interior point methods\\

\noindent {\bf AMS Subject Classification:} 90C30, 49K99, 65K05.

\end{abstract}

\section{Introduction} \label{intro}

Since the 1940s, given its wide range of applications, linear programming problems have been very well studied. 
In particular, the first applications were made in the military sector, in the context of WWII, which boosted the study of numerical solutions of these problems.



In the present date, the state of the art linear programming solvers are interior point methods. These methods can be derived from the idea of minimizing a log-barrier subproblem with Newton method and they can be generalized for solving convex quadratic optimization problems. It is well known that an efficient interior point method should approximately follow the central path, otherwise, the method may fail due to the poor quadratic approximation of the log-barrier function near a non-optimal vertex.

In nonlinear programming, a dual approach to barrier methods are the classical penalty methods. Here, instead of the log-barrier function, one penalizes infeasibility with the $\ell_2$-norm squared. In this case, when dealing with quadratic programming, the subproblems are piecewise quadratic, and Newton method should perform well without the necessity of remaining close to the central path. The main contribution of this paper is investigating this possibility.

Given the enormous success of interior point methods since the work of Karmarkar in 1984, research in penalty methods have been overlooked in favor of barrier-type methods. This is not without merit, however, the goal of this paper is to show that very simple penalty methods can perform similarly to barrier-type methods.

Usually, in interior point methods, a sufficiently interior initial point must be computed in order for the method to perform well. The Mehrotra initial point strategy~\cite{Mehrotra:1992wr} is often the choice, even to the point that many interior point linear programming solvers do not even allow the user to specify a different initial point. This situation can be costly if one is solving a perturbation of a problem that has already been solved, as it happens in many applications. %A particular case is when a sequence of linear programming problems are solved, as in sequential linear programming methods for nonlinear optimization. 
In this case, the solution of a problem can give significant information about the solution of the next problem. Thus, not using the solution of the previous problem as an initial point for the next, is usually a bad decision. Recently some warm-start strategies for interior point methods have been presented, which are competitive with Simplex solvers~\cite{Yildirim:2002iy,John:2007jm}. However, even when an initial solution is given in a warm-started interior point method, some modification of the solution must occur in order to provide to the solver an interior initial point in a neighborhood of the central path. This drawback is not present in penalty methods, as any initial solution can be exploited in its entirety.

There has been many recent developments in penalty methods for nonlinear optimization, in particular, in augmented Lagrangian methods. In this paper, we revisit the convex quadratic programming problem in light of these recent augmented Lagrangian methods in order to propose an efficient method. Our algorithm will follow very closely an interior-point-like framework, in the sense that its core computational work will resort to the computation of interior-point-like Newton directions.

In Section 2, we recall some general augmented Lagrangian results, where we obtain some specialized results in the  convex quadratic case. In Section 3, we present our strategy for solving the augmented Lagrangian subproblem. In Section 4 we show finite convergence of the augmented Lagrangian algorithm for linear and convex quadratic problems. In Section 5, we present our numerical results. Finally, we end the paper with some conclusions and  remarks.\\



\noindent {\bf Notation:} The symbol $\|\cdot\|$ denotes the Euclidean norm in $\R^n$. By $\R^n_+$ we denote the set of vectors in $\R^n$ with all components non-negative. The set of non-negative integers is denoted by $\N$. If $K \subseteq \N$ is an infinite sequence of indexes and $\lim_{k \in K} x^k = x$, we say that $x$ is a limit point of the sequence $\{x^k\}$.



\section{The Augmented Lagrangian method}  \label{ALm}

Let us consider the general nonlinear programming problem in the following form:
\begin{equation} \label{PNL}
\text{Minimize } f(x), \text{ subject to } h(x)=0, \, g(x)\leq0, \, H(x)=0, \, G(x)\leq0,
\end{equation}
where $f:\R^n\to\R, h:\R^n\to\R^m, g:\R^n\to\R^p, H:\R^n\to\R^{\bar{m}}, G:\R^n\to\R^{\bar{p}}$ are smooth functions.

The constraints $h(x)=0$ and $g(x)\leq 0$ are called the lower-level constraints, while the constraints $H(x)=0$ and $G(x)\leq 0$ are called the upper-level constraints. Given Lagrange multipliers approximations $\lambda\in\R^{\bar m}$, $\mu\in\R^{\bar p}_+$ and a penalty parameter $\rho>0$, the Powell-Hestenes-Rockafellar augmented Lagrangian function associated with the upper-level constraints is defined as
\begin{equation}
\label{auglag}
x\mapsto L(x,\rho,\lambda,\mu)=f(x)+\frac{\rho}{2}\left(\left\|H(x)+\frac{\lambda}{\rho}\right\|^2+\left\|\max\left\{0,G(x)+\frac{\mu}{\rho}\right\}\right\|^2\right).
\end{equation}

The augmented Lagrangian method as described in \cite{bmbook}, in each iteration, approximately solves the subproblem of minimizing the augmented Lagrangian function subject to the lower-level constraints.  Therefore, the set $F \equiv \{x\in\R^m\mid h(x)=0, g(x)\leq0\}$ is the feasible set of the subproblems. The Lagrange multipliers approximations are updated at each iteration  in a standard way and  the penalty parameter increases when progress, measured in terms of feasibility and complementarity, is not sufficiently made. More precisely, given the current iterate $x^k\in\R^m$, the current penalty parameter $\rho_k>0$, and the current Lagrange multipliers approximations $\lambda^k\in\R^{\bar{m}}$ and $\mu^k\in\R^{\bar{p}}_+$, a new iteration is computed in the following way:

\begin{itemize}
\item {\bf Step 1 (solve subproblem)}: from $x^k$, find an approximate solution $x^{k+1}$ of the problem:
\begin{equation} \label{subproblemaLA}
\text{Minimize } L(x,\rho_k,\lambda^k,\mu^k), \text{ subject to } x\in F.
\end{equation}
\item {\bf Step 2 (update multipliers)}: compute $\lambda^{k+1}$ in $[\lambda_{\min},\lambda_{\max}]$ and $\mu^{k+1}$ in $[0,\mu_{\max}]$.
\item {\bf Step 3 (update penalty)}: Set $V^{k+1}=\max\{G(x^{k+1}),-\mu^k/\rho_k\}$. If $$\|(H(x^{k+1}),V^{k+1})\|_\infty> \frac{1}{2}\|(H(x^{k}),V^{k})\|_\infty,$$ set $\rho_{k+1}=10\rho_k$, otherwise set $\rho_{k+1}=\rho_k$.
\end{itemize}

One of the most usual rules for updating the multipliers is the first order update in which $\lambda^{k+1}$ is computed as the projection of $\lambda^k+\rho_k H(x^{k+1})$ onto a safeguarded box $[\lambda_{\min},\lambda_{\max}]$ and $\mu^{k+1}$ as the projection of $\mu^k+\rho_k G(x^{k+1})$ onto a safeguarded box $[0,\mu_{\max}]$. Standard first- and second-order global convergence results are proved under weak constraint qualifications, depending whether subproblems are solved approximately up to first- or second-order, respectively. See, for instance, \cite{bhr} for details. If approximate global minimizers are found for the subproblems, one gets convergence to a global minimizer, and that is the main reason for using a safeguarded Lagrange multiplier (see \cite{bmbook}).

In terms of the global convergence theory, the choice of lower- and upper-level constraints can be done arbitrarily, however, the practical behavior of the method depends strongly on the quality of the optimization solver to minimize the augmented Lagrangian function subject to the lower-level constraints. The ALGENCAN implementation\footnote{Freely available at: \url{www.ime.usp.br/~egbirgin/tango}.} considers $F=\{x\in\R^n \mid \ell\leq x\leq u\}$ and penalizes all remaining constraints, using an active-set strategy with the spectral projected gradient choice to leave a non-optimal face, when solving the box-constrained subproblems.

In \cite{seco}, a radical shift of approach was suggested, by penalizing the box constraints and keeping equality constraints as subproblems' constraints. That is,  when solving a problem with constraints $h(x)=0,\,\ell\leq x\leq u$, the authors of \cite{seco} chose $\ell\leq x\leq u$ as the upper level constraints to be penalized and $h(x)=0$ as the lower level constraints. They then explored the simplicity of the KKT system when only equality constraints are present to develop a simple Newton method for solving the subproblems. Surprisingly, this simple approach had a performance compatible with ALGENCAN, a fine-tuned algorithm, on the CUTEst collection.

This approach is very similar to the interior point approach, where simple bound constraints are penalized with the log-barrier function, and Newton method is used for solving the subproblem with equality constraints. This similarities motivated us to compare the interior point and the augmented Lagrangian approaches in the simplest context, that is, when the constraints are linear and the objective function is convex and quadratic.

We will present an augmented Lagrangian algorithm for convex quadratic programming on the lines of the developments in \cite{seco}. That is, we are interested in the following problem:
\begin{equation} \label{PL}
\text{Minimize } \frac{1}{2}x^TQx+c^Tx \text{ subject to } Ax=b, \;\;\; \ell \leq x \leq
u,
\end{equation}
where $c,\ell,u \in \R^n$ with $\ell < u$, $b \in \R^m$, $A \in \R^{m \times n}$, and a positive semidefinite symmetric $Q\in\R^{n\times n}$ are given. We assume that $m<n$ and $A$ is full rank. %\textcolor{red}{Nao seria isso? matrix, $d^TQd \geq 0$ for all non null $d$ in the kernel of $A$.} 
From now on, the set $F$ is defined by the points that fulfill the equality constraints, that is,
$$F=\{x\in\R^n\mid Ax=b\}.$$



The algorithm presented in this section is a particular
case of the augmented Lagrangian method described previously, when applied to the convex quadratic programming problem \eqref{PL}, penalizing the box constraints and considering the equality constraints as the lower level constraints. An approximate solution for the subproblem will be one that satisfies the first-order stationarity conditions up to a given tolerance, that is, the norm of the residual of the corresponding nonlinear system of equations is at most the given tolerance. Due to the convexity of the problem, this will be enough to obtain convergence to a global minimizer.



Since in this approach there are no equality constraints to be penalized, we will use a slightly different notation than the one presented in the general algorithm. We denote by $\lambda\in\R^m$ a Lagrange multiplier approximation associated with constraints $x\in F$, and by $\mu_{\ell}\in\R^n_+$ and $\mu_u\in\R^n_+$ the Lagrange multipliers approximations associated with constraints $-x+\ell\leq 0$ and $x-u\leq 0$, respectively. Thus, given $\rho$, $\mu_{\ell}$  and $\mu_u$, the Augmented Lagrangian function used in this paper is defined by
\begin{equation} \label{auglag}
L(x,\rho,\mu_{\ell},\mu_u)=\frac{1}{2}x^TQx+c^Tx+\frac{\rho}{2}\left(\left\|\max\left\{-x+\left(\ell+\frac{\mu_{\ell}}{\rho}\right),0\right\}\right\|^2+ \left\|\max\left\{x-\left(u-\frac{\mu_u}{\rho}\right),0\right\}\right\|^2\right),
\end{equation}
and the subproblem of interest is
\begin{equation} \label{subprobLA}
\text{Minimize } \, L(x,\rho,\mu_{\ell},\mu_u) \, \text{ subject to } \, Ax=b,
\end{equation}
whose optimality conditions are $\nabla L(x,\rho,\mu_{\ell},\mu_u)+ A^T \lambda=0$ and $Ax=b$. Therefore we say that a  point $x \in F$ is an $\epsilon$ approximate solution of \eqref{subprobLA} if there exists $\lambda$ such that  $\|\nabla L(x,\rho,\mu_{\ell},\mu_u)+ A^T \lambda \| \leq \epsilon$.  Precisely, the algorithm is set out as follows.

\begin{algorithm}[H]
  \caption{Augmented Lagrangian algorithm}
  \label{ALalg}
  \begin{algorithmic}
\State \noindent  \textbf{Step 0 (initialization)}: \\ Let $\rho_0=1$ and $\varepsilon \geq 0$ and set $k\leftarrow0$. Compute $(x^0,\lambda^0,\mu_{\ell}^0,\mu_u^0)\in F\times\R^m\times\R^n_+\times\R^n_+$.\\

\State \noindent \textbf{Step 1 (stopping criterion)}:\\ Set the primal residual $r_P=\max\{0,\ell-x^k,x^k-u\}$, the dual residual $r_D=Qx^k+c+A^\T\lambda^k-\mu_{\ell}^k+\mu_u^k$ and the complementarity measure $r_C=(\min(\mu_{\ell}^k,x^k-\ell),\min(\mu_u^k,u-x^k))$. Stop if $\|(r_P,r_D,r_C)\|_\infty\leq\varepsilon$.\\

        
    \State  \noindent \textbf{Step 2 (solve subproblem)}:\\
    Find $x^{k+1} \in F$,  with corresponding approximate Lagrange multiplier $\lambda^{k+1}$, an $\varepsilon_{k+1}$ approximate solution   of the subproblem
\begin{equation}\label{subprobLAk}
\text{Minimize } \,  L(x,\rho_k,\mu_{\ell}^k,\mu_u^k) \, \text{ subject to } \, Ax=b.
\end{equation}

    \State  \noindent \textbf{Step 3 (update multipliers)}:\\    
    Compute $\mu_{\ell}^{k+1},\mu_u^{k+1}\in[0,\mu_{\max}]$, for some fixed $\mu_{\max}>0$. This procedure can be a first- or a second-order one and will be specified afterwards.\\


    \State  \noindent \textbf{Step 4 (update penalty)}: If
    $$\left\|\max\left\{-\frac{\mu_{\ell}^k}{\rho_k},-\frac{\mu_u^k}{\rho_k},x^{k+1}-u,\ell-x^{k+1}\right\}\right\|_{\infty}>
    \frac{1}{2}\left\|\max \left\{-\frac{\mu_{\ell}^{k-1}}{\rho_{k-1}},-\frac{\mu_u^{k-1}}{\rho_{k-1}},x^{k}-u,\ell-x^{k} \right\}\right\|_{\infty}$$ set $\rho_{k+1}=10 \rho_k,$ otherwise set $\rho_{k+1}=\rho_k$.\\
    	%,-\frac{\mu_u^{k-1}}{\rho_{k-1}},x^{k}-u,\ell-x^{k}

    \State  \noindent \textbf{Step 5. (repeat)}: Update $k \leftarrow k+1$ and go to Step~1.\\

  \end{algorithmic}
\end{algorithm}

\subsection{Lagrange multipliers update}

We will consider two options for the Lagrange multiplier update in Step 3 of Algorithm \ref{ALalg}. The first-order update will be defined as $\mu_{\ell}^{k+1}=\max\{0,\mu_{\ell}^k+\rho_k(-x^k+\ell)\}$ and $\mu_u^{k+1}=\max\{0,\mu_u^k+\rho_k(u+x^k)\}$. This classic rule correspond to make a steepest descent iteration for the dual function, see \cite{bertsekasCOA}. Using this update in all iterations, it has been proved in \cite{LAgnep} that the sequence $\{(\mu_{\ell}^k,\mu_u^k)\}$ is bounded. More specifically, the first-order Lagrange multiplier update is bounded for problems that satisfy the quasinormality constraint qualification (which includes linear constraints). This indicates that this update is acceptable for sufficiently large $\mu_{\max}$, hence this parameter can be dropped.



Our second-order Lagrange multiplier update is used when dealing with separable strictly convex quadratic problems. The update formula is obtained according to the ideas presented in \cite{yaxyuan,bertsekas,buys}. For the derivation of this rule it is used that, fixing $\rho$, the solution of subproblem  \eqref{subprobLA} is unique for each $\mu_{\ell}$ and $\mu_u$, and so we can define $x(\mu)$ as the solution to this problem. Thus, the second order update of the multipliers consists of taking the positive part of the solution of applying one iteration of Newton's method to maximize the associated dual function \cite{bertsekas,buys} or, in accordance with \cite{yaxyuan}, to the  system defined by the constraints in order to approach feasibility.

With the purpose of presenting the update formula  we need to introduce a notation to refer to the coordinates associated with the active or infeasible displaced box constraints at a point $x$.  Thus let us define $\alpha_{\ell}^k(x)=\left\{i: x_i\leq \ell_i+\frac{[\mu_{\ell}^k]_i}{\rho_k}\right\}$, $\alpha_{\ell}^k=\alpha_{\ell}^k(x^{k+1})$, $\alpha_u^k(x)=\left\{i: x_i\geq u_i-\frac{[\mu_u^k]_i}{\rho_k}\right\}$ and $\alpha_u^k=\alpha_u^k(x^{k+1})$. The coordinates related to the constraints without the displacement will be denoted without the indexes $k$, that is, $\alpha_{\ell}(x)=\left\{i: x_i\leq \ell_i \right\}$ and $\alpha_u(x)=\left\{i: x_i\geq u_i \right\}$.



Moreover, we will need to refer to parts of vectors and matrices corresponding to these coordinates.
 Let us define $\mathcal{I}_{\ell}^k$  as the $|\alpha_{\ell}^k|\times n$ matrix formed by the $i$-th line of the $n\times n$ identity, $i\in \alpha_{\ell}^k$. In the same way we define $\mathcal{I}_u^k$.  The notation $v_{\alpha_{\ell}^k}\in\R^{|\alpha_{\ell}^k|}$ corresponds to the vector with components $v_i, i\in \alpha_{\ell}^k$ of the vector $v\in\R^n$. Similarly for $\alpha_u^k$. Since the Newton method is a local procedure, at  $x^{k+1}$,  $\mu_{\ell}^k$, $\mu_u^k$, and $\rho_k$, the formula would be the same as the positive part of the one applied to the problem
 \begin{equation} \label{QPRestAtiv}
 \text{Minimize } \frac{1}{2}x^TQx+c^Tx \text{ subject to } Ax=b, \;\;\; x_{\alpha_{\ell}^k}=\ell_{\alpha_{\ell}^k}, \, \text{ and } \, x_{\alpha_u^k}=u_{\alpha_u^k},
 \end{equation}
 when penalizing the active box constrains.
 
 Let $Z_k=\left(\begin{array}{c} -\mathcal{I}_{\ell}^k\\\mathcal{I}_u^k\\A\end{array}\right)$ and $M_k=Z_k(Q+H^k)^{-1}Z_k^\mathtt{T}$, where $H^k$ is the Hessian of $$\frac{\rho_k}{2}\left(\left\|\max\left\{-x+\left(\ell+\frac{\mu_{\ell}^k}{\rho_k}\right),0\right\}\right\|^2+
\left\|\max\left\{x-\left(u-\frac{\mu_u^k}{\rho_k}\right),0\right\}\right\|^2\right)$$ at $x^{k+1}$, taken in the semismooth sense, namely, $H^k$ is a diagonal matrix with its $i$-th entry equal to $0$ if $\ell_i+\frac{[\mu_{\ell}^k]_i}{\rho_k}<x_i^{k+1}<u_i-\frac{[\mu_u^k]_i}{\rho_k}$ and equal to $\rho_k$ otherwise. To update the Lagrange multiplier approximations $\mu^k_{\ell}$ and $\mu^k_u$, we solve the linear system 
\begin{equation}\label{sist2orderupdate}
M_k\left(\begin{array}{c}d^1\\d^2\\d^3\end{array}\right)=\left(\begin{array}{c}\ell_{\alpha_{\ell}^k}-x^{k+1}_{\alpha_{\ell}^k}\\x^{k+1}_{\alpha_u^k}-u_{\alpha_u^k}\\b-Ax^{k+1}\end{array}\right),
\end{equation}
where $d^1\in\R^{|\alpha_{\ell}^k|}, d^2\in\R^{|\alpha_u^k|}$, and $d^3\in\R^m$.  We then define $[\mu_{\ell}^{k+1}]_{\alpha_{\ell}^k}=\max\{0,[\mu_{\ell}^{k}]_{\alpha_{\ell}^k}+d^1\}$ and $[\mu_u^{k+1}]_{\alpha_u^k}=\max\{0,[\mu_u^{k}]_{\alpha_u^k}+d^2\}$, keeping $[\mu_{\ell}^{k+1}]_i=[\mu_{\ell}^k]_i$ and $[\mu_u^{k+1}]_i=[\mu_u^k]_i$ for the remaining indexes. This is the standard second-order Lagrange multiplier update when considering that all constraints are penalized. However, by following the approach of \cite{yaxyuan} but considering constrained subproblems, we arrived at the same formula.

When $Q$ is diagonal and positive definite (separable and strictly convex problem), the expression for $M_k$ can be easily computed explicitly, and this is the case where we may use the second-order update. If $Q$ is not positive definite or the inverse of $Q+H^k$ is not easily computable, we advocate the use of the first-order update only. In our numerical  implementation we also used the first-order update if the solution of \eqref{sist2orderupdate} is not accurate.



%In our numerical implementation of the algorithm, we consider the second-order update when dealing whith strictly convex problems with diagonal hessian. 


In \cite{yaxyuan} the author presented a local superlinear convergence for the second order update rule,  unfortunately, he said that he was not able to obtain an general result. Considering this, we tested our algorithm using the second-order update with safeguards in all iterations and also making use of this formula only when we had some indication that we were approaching the solution. The most efficient version of our algorithm uses the first-order update unless two consecutive iterations are such that  $\alpha_{\ell}^{k-1}(x^{k-1})=\alpha_{\ell}^{k}(x^{k})$ and $\alpha_u^{k-1}(x^{k-1})=\alpha_u^{k}(x^{k})$. 

Since we will prove that the sequence $x^k$ converges to $x^*$, a solution of the problem \eqref{PL}, it is reasonable to expect  that the number of iterations of the method in which second-order updates with $\alpha_{\ell}^{k}(x^{k}) \neq \alpha_{\ell}(x^{*})$ or  $\alpha_u^{k}(x^{k}) \neq \alpha_u(x^{*})$ are made are finite. Therefore, the behavior of the hybrid method must be very similar with the method that uses the standard first-order update  until $\alpha_{\ell}^{k}(x^{k}) = \alpha_{\ell}(x^{*})$ and  $\alpha_u^{k}(x^{k}) = \alpha_u(x^{*})$.





\subsection{Convergence Analysis}

In order to analyze the asymptotic behavior of the algorithm, 
we consider that it only stops at an iteration $k_0$ when an exact solution is found, that is, we consider $\varepsilon=0$ in the stopping criterion. Even in this case, we consider that an infinite sequence is generated with $x^{k}=x^{k_0}$ for all $k > k_0$.

In augmented Lagrangian approaches where the box constraints $ \ell \leq x \leq
u$ are not penalized, it is straightforward that the subproblems are well defined and
that the generated sequence remains in a compact set. The following
lemmas ensure that these properties hold for Algorithm \ref{ALalg}, even though box constraints are not necessarily preserved. This is a consequence of dealing with quadratic convex problems. We start by showing well-definiteness:



\begin{lemma} \label{teobemdef} If $F \neq \emptyset$ then subproblem \eqref{subprobLAk} is always well defined.
\end{lemma}

\noindent {\it Proof.} It is easy to see that the objective function of the subproblem, $x\mapsto L(x,\rho_k,\mu_{\ell}^k,\mu_u^k)$, tends to
infinity when the norm of $x$ goes to infinity, that is, it is coercive, and the result follows from a well known existence result \cite{bertsekas}.\halmos



The next result shows that it is actually
possible to ensure that a sequence generated by Algorithm \ref{ALalg} lies in a compact set:

\begin{lemma} \label{ficacompato} If $F \neq \emptyset$ then a sequence
$\{x^k\}$ generated by Algorithm \ref{ALalg} lies in a compact
set.
\end{lemma}

\noindent {\it Proof.} By Lemma \ref{teobemdef}, the sequence $\{x^k\}$ is well defined. Let us show that
there is a single compact set containing all iterates $x^k$.




Let
$$L(x,\rho_k,\mu_{\ell}^k,\mu_u^k)= \frac{1}{2}x^TQx+\sum_{i=1}^n L_i^k(x_i),$$
where
\begin{equation}\label{defLik}
L_i^k(x_i)=c_ix_i+\frac{\rho_k}{2}\left(\max\left\{\ell_i-x_i+\frac{[\mu_{\ell}^k]_i}{\rho_k},0\right\}^2+
\max\left\{x_i-u_i+\frac{[\mu_u^k]_i}{\rho_k},0\right\}^2\right).
\end{equation}


For each function $L_i^k(x_i)$ we have that
\begin{equation}\label{defLi}
\begin{array}{ccl}
L_i^k(x_i)&\geq& c_i
x_i+\frac{\rho_0}{2}\left(\max\left\{\ell_i-x_i+\frac{[\mu_{\ell}^k]_i}{\rho_k},0\right\}^2+
\max\left\{x_i-u_i+\frac{[\mu_u^k]_i}{\rho_k},0\right\}^2\right)\\[2pt]
&\geq& c_i x_i+\frac{\rho_0}{2}\left(\max\left\{\ell_i-x_i,0\right\}^2+
\max\left\{x_i-u_i,0\right\}^2\right) \equiv L_i(x_i).
\end{array}
\end{equation}


Note that $L_i(x_i)$ does not depend on $k$ and, since
\begin{equation}\label{limLi}
L_i(x_i) \geq \left\{
\begin{array}{l}
c_i \ell_i - \frac{c_i^2}{2\rho_0}, \; \text{ if } \; c_i\geq 0,\\[5pt]
c_i u_i - \frac{c_i^2}{2\rho_0}, \; \text{ if } \; c_i\leq 0,
\end{array}
\right.
\end{equation}
it is bounded below.

On the other hand, the function $L_i^k(x_i)$ is bounded above by
$$L_i^k(x_i)\leq c_ix_i+\frac{\rho_k}{2}h_i(x_i),$$
where
$$h_i(x_i) \equiv \max\left\{\ell_i-x_i+\frac{\mu_{\max}}{\rho_0},0\right\}^2+
\max\left\{x_i-u_i+\frac{\mu_{\max}}{\rho_0},0\right\}^2.$$

Let $\bar{x} \in \R^n$ be a feasible
point for the subproblems, that is $A\bar{x}=b$. Denoting $\bar{h}=
\sum_{i=1}^n h_i(\bar{x}_i)$ we have that
\begin{equation}\label{eq3xkcompacto}
L(\bar{x},\rho_k,\mu_{\ell}^k,\mu_u^k) \leq \frac{1}{2} \bar{x}^T Q \bar{x}+ c^T\bar{x}+\frac{\rho_k}{2} \bar{h}.
\end{equation}


Now, suppose by contradiction that the sequence $\{x^k\}$ is unbounded.
In this case there would be an index $i$ and an infinite set $K
\subset \N$ such that
$$\lim_{k \in K} x^k_i=- \infty \;\; \text{ or } \lim_{k \in K} x^k_i=
\infty.$$
Without loss of generality we assume that $\lim_{k \in K}
x^k_1= \infty$.

By \eqref{limLi}  there exists $\bar{L}$ such that $\bar{L} < \sum_{i=2}^n L_i^k(x_i)$.
Therefore,
\begin{equation}\label{eq1xkcompacto}
\bar{L}+c_1x_1+ \frac{\rho_k}{2} \max\left\{x_1-u_1,0\right\}^2 <
\frac{1}{2} x^T Q x+\sum_{i=1}^n L_i^k(x_i)=L(x,\rho_k,\mu_{\ell}^k,\mu_u^k).
\end{equation}

Combining \eqref{eq3xkcompacto} and  \eqref{eq1xkcompacto} we will show that $L(x^k,\rho_k,\mu_{\ell}^k,\mu_u^k)$ is significantly greater than  $L(\bar{x},\rho_k,\mu_{\ell}^k,\mu_u^k)$. In order to do this we will show that,   given any $p>0$, for $x_1$ large enough
\begin{equation}\label{eq15xkcompacto}
 \frac{1}{2} \bar{x}^T Q \bar{x}+ c^T\bar{x}+\frac{\rho_k}{2} \bar{h}+p \leq \bar{L}+c_1x_1+ \frac{\rho_k}{2} \max\left\{x_1-u_1,0\right\}^2.
\end{equation}
Since $\rho_k \geq \rho_0$, if $x_1^k > u_1+ \sqrt{\bar{h}}$, \eqref{eq15xkcompacto}  holds if  
\begin{equation}\label{eq2xkcompacto}
\frac{\rho_0}{2} [(x_1-u_1)^2- \bar{h}]+c_1x_1+\bar{C} \geq 0,
\end{equation}
where $\bar{C}=\bar{L}-\frac{1}{2} \bar{x}^T Q \bar{x}- c^T\bar{x} -p$.



Defining   $\Delta=c_1^2-2\rho_0 \bar{C}+ \rho_0^2 \bar{h}-2\rho_0c_1u_1$ we have that, whenever $\Delta < 0$, \eqref{eq2xkcompacto} is satisfied for all $x_1$. If $\Delta >0$, \eqref{eq2xkcompacto} holds for $x_1 \geq  u_1+\frac{-c_1+\sqrt{\Delta}}{\rho_0}$. Therefore, $x_1^k > \max\{u_1+\frac{-c_1+\sqrt{\Delta}}{\rho_0}, u_1+ \sqrt{\bar{h}}\}$ ensures that $L(x^k,\rho_k,\mu_{\ell}^k,\mu_u^k) > L(\bar{x},\rho_k,\mu_{\ell}^k,\mu_u^k) + p.$
Since $p$ could be arbitrarily large, this contradicts  the fact that $x^k$ is an approximate minimizer of the subproblem. \halmos



We next recall the standard global convergence theory of Algorithm \ref{ALalg} as described in \cite{bmbook,abms}. The difference is that convexity implies that stationary points are global solutions. The main result is that every limit point of a sequence
generated by the algorithm is a solution of the original problem \eqref{PL}, provided the feasible set is non-empty.


\begin{theorem} \label{teoachasol} Assume that
the feasible set of problem \eqref{PL} is non-empty, then every
limit point of a sequence $\{x^k\}$ generated by Algorithm
\ref{ALalg} is a solution of problem \eqref{PL}.
\end{theorem}

\noindent {\it Proof.} First, note that the linearity of the constraints trivially implies that the constant rank constraint qualification (CRCQ)~\cite{crcq} holds. In particular, weaker constraint qualifications such as CPLD~\cite{cpld,rcpld}, CPG~\cite{cpg} or CCP~\cite{ccp} also hold.


By \cite[Corollary 6.2]{abms} we have that  if $x^*$ is a
limit point of the sequence $\{x^k\}$ then it is a stationary point
of the problem
\begin{equation} \label{probviabil}
\text{Minimize } \left(\left\|\max\left\{\ell-x,0\right\}\right\|^2+
\left\|\max\left\{x-u,0\right\}\right\|^2\right) \text{ subject to }
Ax=b.
\end{equation}
Since this problem is convex, we can ensure that $x^*$ is a global
minimizer of \eqref{probviabil}. Thus, by the non-emptiness of the feasible set, $x^*$ is feasible for problem \eqref{PL}.

Hence, by \cite[Corollary 6.1]{abms}, $x^*$ is stationary for \eqref{PL}. By convexity, $x^*$ is a solution of problem \eqref{PL}. \halmos

Note that combining Lemma \ref{ficacompato} and Theorem \ref{teoachasol}, any sequence generated by the algorithm must have at least one limit point, which must be a solution of problem \eqref{PL}. Hence, one solution of the original problem is necessarily found by the algorithm. It is also a trivial consequence of Theorem \ref{teoachasol} that if problem \eqref{PL} has a unique solution, than a sequence generated by the algorithm is necessarily convergent to the solution.



\section{Solving the subproblem}  \label{basicmethod}

In this section we discuss how to solve  subproblem
\eqref{subprobLAk} of the Augmented Lagrangian method. Our main idea consists in using a Newton-type strategy. This approach is motivated by the fact that the constraints are linear and the objective function is piecewise quadratic. More precisely, we will solve the subproblem by Newton's method with exact line search.

When solving the subproblem, the indexes $k$ of $\rho_k$, $\mu_{\ell}^k$ , $\mu_u^k$, and $\varepsilon_{k+1}$
are fixed in \eqref{subprobLAk}. Therefore, to simplify the notation,
we will suppress these indexes and redefine the constants
$\ell=\ell+\frac{\mu_{\ell}^k}{\rho_k}$ and   $u=u-\frac{\mu_u^k}{\rho_k}$.
Thus, throughout this section, we consider the objective function of
the subproblem as
$$L(x)=\frac{1}{2}x^TQx+c^Tx+\frac{\rho}{2}\left(\left\|\max\left\{\ell-x,0\right\}\right\|^2+
\left\|\max\left\{x-u,0\right\}\right\|^2\right),$$
which must be minimized subject to $x\in F$.

%We will consider, from now on, that $Q$ is positive definite. If $Q$ is only positive semidefinite, our approach can still be carried out, but for the well-definiteness of the Newton direction one must choose a suitable regularization strategy.

In the following, we consider $x^k$ as the point obtained in the $k$-th iteration of the algorithm for solving the subproblem, thus not associated to the point $x^k$ generated by the external algorithm.

\begin{algorithm}[H]
	\caption{ (for solving the subproblems)}
	\label{solvesubp}
	\begin{algorithmic}
		\State \noindent  \textbf{Step 0 (initialization)}: \\ Let $\varepsilon \geq0$, $\varepsilon_{reg}>0$ and set $k\leftarrow0$. Set $(x^0,\lambda^0)\in F\times\R^m$ as the current outer iterate.\\
		
		\State \noindent \textbf{Step 1 (stopping criterion)}:\\ Set $r=Qx^k+c+A^\T\lambda^k-\rho\max\{\ell-x^k,0\}+\rho\max\{x^k-u,0\}$ and stop if $\|r\|_\infty\leq\varepsilon$.\\
		
		
		\State  \noindent \textbf{Step 2 (Regularization)}:\\
		Define $H^k$ as the diagonal matrix such that $H^k_{ii}=0$ if $\ell_i<x_i^k<u_i$, and $H_{ii}^k=\rho$, otherwise. If $\left(\begin{array}{cc}Q+H^k&A^\mathtt{T}\\A&0\end{array}\right)$ is singular, set $ H^k=H^k+\varepsilon_{reg}I$.\\
		
		
		\State  \noindent \textbf{Step 3 (compute Newton direction)}:\\
		Solve the linear system
		\begin{equation}\label{newton}
		\left(\begin{array}{cc}Q+H^k&A^\mathtt{T}\\A&0\end{array}\right)\left(\begin{array}{c}x^{k+1}_{trial}\\ \lambda^{k+1}\end{array}\right)=\left(\begin{array}{c}-c+v^k+\varepsilon_{reg}x^k\\ b\end{array}\right),
		\end{equation}
		where $v_i^k=\rho u_i$ if $x_i^k\geq u_i$, $v_i^k=\rho \ell_i$, if $x_i^k\leq \ell_i$, and $v_i^k=0$ otherwise.
		\State  \noindent \textbf{Step 4 (line search)}:\\
		Set $d^k=\gamma(x^{k+1}_{trial}-x^k)$, where $\gamma=\min\left\{1,\frac{100}{\|x^{k+1}_{trial}-x^k\|}\right\}$. Find $t^*$ as the global minimizer of the one-dimensional piecewise quadratic $t\mapsto L(x^k+td^k)$ and compute $t_k$ as the projection of $t^*$ onto the safeguarded interval $[-\min\{\|d^k\|,10^{-6}\},1+\|d^k\|]$.\\
		
		\State  \noindent \textbf{Step 5. (update and repeat)}: Set $x^{k+1}=x^k+t_kd^k$. Update $k \leftarrow k+1$ and go to Step~1.\\
		
	\end{algorithmic}
\end{algorithm}

The linear system \eqref{newton} finds a point $x^{k+1}_{trial}$ which is the solution of the regularized problem
\begin{equation}\label{regsubprob}
\text{Minimize } \nabla L(x^k)^T (x-x^k)+ \frac{1}{2} (x-x^k)^T \nabla^2 L(x^k) (x-x^k)+
\frac{\varepsilon_{reg}}{2}\left\|x-x^k \right\|^2  \text{ subject to }  Ax=b.
\end{equation}
If $\varepsilon_{reg}=0$, we obtain the standard Newton direction for the KKT system of the subproblem. Since we are assuming that the starting point is feasible, we have that the direction $d^k$ lies in the kernel of $A$. Therefore, theoretically, all the iterandos are feasible. If feasibility deteriorates due to numerical issues, the new direction obtained tends to recover it.
The direction is re-scaled to avoid a direction with norm greater than $100$. Once an exact line search is employed, this do not have any theoretical influence.  


Let $N$ be a matrix such that its columns form an orthonormal basis of the null-space of $A$. Since the number of possible $H^k$ is finite, we have that the eigenvalues of $N^T [Q+H^k] N$ lie in a positive interval $[\sigma_{\min}, \sigma_{\max}]$. This means that $d^k$ is a descent direction for $L(x)$ at $x^k$. The minimization of $L(x^k+td^k)$ is done by a sequence of straightforward minimization of (smooth) unidimensional convex quadratics. The safeguarded projection is considered to avoid numerical errors when the Newton direction is not accurately computed.


 Next lemma shows that a sufficient decrease is obtained at each iteration of Algorithm \ref{solvesubp}. 



\begin{lemma}
	Let $\sigma:= \min \left\{\frac{\sigma_{\min}}{2}, \frac{1}{2+\sigma_{\max}}
	\right\}$. For all $k$ we have that
	\begin{equation}\label{decsuf}
	L(x^{k+1}) \leq  L(x^k)-\frac{\sigma^2}{2M}\|d^k\|^2.
	\end{equation}
\end{lemma}
\noindent {\it Proof.}  
Since $x=x^k$ is feasible to (\ref{regsubprob}),  we have that
$$ \nabla L(x^k)^T d^k + \frac{1}{2}(d^k)^T [Q+H^k] d^k \leq 0.$$
Therefore,
\begin{equation} \label{condangsuf}
\nabla L(x^k)^T d^k \leq -\frac{\sigma_{\min}}{2} \|d^k\|^2 \leq - \sigma \|d^k\|^2.
\end{equation}
Condition \eqref{condangsuf} ensures that $d^k$ satisfies a
sufficient descent criterion.

Since the  function $L(x)$ is defined by a finite quantity of quadratics, its gradient is Lipschitz. That is, there exists $M$ such that
$$\| \nabla L(x)-\nabla L(y) \| \leq M \|x-y\|,$$
therefore
$$L(y) \leq L(x)+ \nabla L(x)^T(y-x)+\frac{M}{2}\|x-y\|^2.$$
So, by \eqref{condangsuf}, for all $t>0$,
\begin{eqnarray*}
	L(x^k+td^k) &\leq& L(x^k)+ t\nabla L(x^k)^Td^k+\frac{M}{2}t^2 \|d^k\|^2\\
	&\leq& L(x^k)- \sigma t \|d^k\|^2+\frac{M}{2}t^2 \|d^k\|^2\\
	&\leq& L(x^k)- t \left(\sigma -\frac{M}{2}t \right)\|d^k\|^2.\\
\end{eqnarray*}
Using that $t^k$ is the minimizer of $L$ along the direction  $d^k$ and considering $t=\frac{\sigma}{M}$ in the previous expression we have that 
$$L(x^{k+1}) \leq L\left(x^k+\frac{\sigma}{M} d^k\right) \leq L(x^k)-\frac{\sigma^2}{2M}\|d^k\|^2.$$
\halmos

Since $x^{k+1}_{trial}$ is the solution of the linearly constrained problem
(\ref{regsubprob}),
$$P_{F}(x^{k+1}_{trial}-[Q+H^k](x^{k+1}_{trial}-x^k)-\nabla L(x^k))-x^{k+1}_{trial}=0.$$
Using the triangular inequality and fact that the projections
are non-expansive we have:
\[
\|P_F (x^k - \nabla L(x^k))-x^k\|= \|P_F (x^k - \nabla L(x^k))-x^k -P_{F}(x^{k+1}_{trial}-[Q+H^k](x^{k+1}_{trial}-x^k)-\nabla L(x^k))+x^{k+1}_{trial}\|
\]
\[
\leq \|x^k-x^{k+1}_{trial} + [Q+H^k](x^{k+1}_{trial}-x^k)\| + \|x^k-x^{k+1}_{trial}\| \leq (2+\sigma_{max})\|x^{k+1}_{trial}-x^k\|.
\]


So, we have that
\begin{equation} \label{bronte}
\sigma \|P_{F}(x^k - \nabla L(x^k))-x^k\| \leq  \|x^{k+1}_{trial}-x^k\|.
\end{equation}
Condition \eqref{bronte} shows that small directions allowed only if
$x^k$ is close to a solution of \eqref{subprobLAk}.


Once again, in order to analyze the asymptotic behavior, we will consider that the Algorithm~\ref{solvesubp} stops only if there is $k_0$ such that $x^{k_0}$ is an exact
solution of \eqref{subprobLAk}. In this case, we declare that  $x^{k}=x^{k_0}$ for all $k \geq k_0$.


\begin{theorem} \label{ptolimitesolucao}
All the limit points of the sequence $\{x^k\}$, generated by Algorithm  \ref{solvesubp}, are solutions of the subproblem \eqref{subprobLAk}.
\end{theorem}

\noindent {\it Proof.} By \eqref{decsuf}, 
$$L(x^{l+1})-L(x^0)=
\sum_{k=0}^l(L(x^{k+1})-L(x^k))$$
$$\le -\frac{\sigma^2}{2M}\sum_{k=0}^l\|d^k\|^2.$$


Since $L(x)$ is continuous and coercive, we have that $\{L(x^k)\}$ is bounded below, so
the series $\sum_{k=0}^\infty \|d^k\|^2$ is convergent, and thus,
$\{\|d^k\|\}$ converges to zero. Moreover, for $k$  large enough, $d^k=x^{k+1}_{trial}-x^k$.

By \eqref{bronte} we have that, if $x^*$ is a limit point $\{x^k\}$,
it satisfies the L-AGP optimality condition \cite{ahm}. Since the
constraints of \eqref{subprobLAk} are linear, we have that $x^*$ is a
stationary point for \eqref{subprobLAk}. Moreover, due to the
convexity of \eqref{subprobLAk}, we have that $x^*$ is a solution of
\eqref{subprobLAk}. \halmos


Once again, the coercivity of $L(x)$ implies that  the sequence
generated by Algorithm \ref{solvesubp} remain in a compact set. Thus, there exists at least one limit point of $\{x^k\}$, which, by Theorem~\ref{ptolimitesolucao}, is a solution of the subproblem
\eqref{subprobLAk}. Moreover, if the   subproblem \eqref{subprobLAk} has an unique solution then the sequence $\{x^k\}$ converges to it.


We end this section with a technical result  discussing the finite convergence of the subproblems. Sufficient conditions in the original problem that guarantee the hypotheses of the following theorem will be discussed in the next section.


\begin{theorem}\label{convfinitasubprob}
	Suppose that the subproblem \eqref{subprobLAk} has a unique solution $x^*$ and that
	$\ell_i \neq x^*_i \neq u_i$ for all $i$,	then  $x^{k}=x^*$ for $k$ large enough.
\end{theorem}

\noindent {\it Proof}  Since $\ell_i \neq x^*_i \neq u_i$ for all $i$, $L(x)$ is a convex smooth quadratic function $q(x)$ in a neighborhood of $x^*$. Since \eqref{subprobLAk} has an unique solution, $x^*$ is also the unique solution of the quadratic problem 
$\text{minimize } q(x) \text{ subject to } Ax=b$. Thus, we have that $d^T\nabla^2 L(x)d>0$ for all nonzero $d$ such that
$Ad=0$ and $x$ is close to $x^*$. 
By Theorem \ref{ptolimitesolucao} we have that the whole sequence $\{x^k\}$ converges to $x^*$, so, for $k$ large enough, $Q+H^k=\nabla^2 L(x^k)$. Therefore, in this situation, $d^k$ is the Newton step for a strictly convex quadratic problem, and so $x^{k+1}=x^k+d^k$ is the solution of \eqref{subprobLAk}. 
\halmos


\section{Finite Convergence}\label{finiteconvergence}

In this session we will show that under mild hypotheses the Augmented Lagrangian algorithm has finite convergence. This is an interesting property of the augmented Lagrangian method that is not enjoyed by pure interior point methods, that is, results of this type are available for interior point methods only by the addition of particular procedures employed at the end of the execution. See \cite{bixby,vavasis,Ye1992}. 

In some sense, our results are similar to the finite termination of a Newton method for piecewise linear systems described in \cite{yunier}.  However, our approach is intrinsically linked to the optimization framework, and therefore it uses the primal structure more prominently than the dual. Indeed, the finite convergence of the outer iterations requires that the subproblems are exactly solved  but it is independent of how to obtain this. On the other hand, the convergence of the subproblem is closely linked to the use of a Newtonian strategy. Even so, our results use a line search strategy to decrease the objective function of the subproblem, which is essentially different from \cite{yunier}. With this approach, we can solve problems where $Q$ is not positive definite, using a regularization scheme that is not considered in \cite{yunier}. Therefore, we can analyze the finite convergence even for linear programming problems.

For linear programming problems where equality constraints are penalized, it is known that the Augmented Lagrangian method converges to the solution in  finite time, see Proposition 5.2.1 of \cite{bertsekasCOA}. In \cite{bertsekasCOA} the penalization of inequalities is also considered, however, it is done by adding slack variables and penalizing the new equalities constraints over the bound constrained set. Since we are actually proposing to not include the box constraints when solving the subproblem, we can not use this approach. 


The finite convergence result of \cite{bertsekasCOA} arises from the fact that, by interpreting the Augmented Lagrangian with the first order update as a proximal point strategy, the exact Lagrange multipliers are obtained in a finite number of iterations. Since we penalize inequality constraints, we need to ensure that the iterations will be in a region where the function is sufficiently smooth as we approach the solution. We obtain this under the strictly complementary and non-degenerecency hypothesis,  that will be discussed latter. More references of the use of Augmented Lagrangian methods for LP problems penalizing equality  constraints can be found in \cite{GulerLALP1992,WrightALLP,RefMaculan}.


We start  showing the finite convergence for LP problems under mild hypotheses. After that,  we present some algorithmic hypotheses that will be used in the finite convergence analysis  when dealing with strictly convex quadratic programming  and using the second order Lagrange multipliers update is used. We will discuss the plausibility of  the assumptions showing that, under certain conditions, they are  reasonable. 




Our first   hypothesis requires the standard definition of regularity and strict complementarity.


%We end this section by showing that the algorithm finds a solution in finite time provided that subproblems are solved exactly ($\varepsilon_k=0$ for all $k$) and that the sequence $\{x^k\}$ converges to a regular and strictly complementary solution. 

%The next results shows that the whole sequence $\{x^k\}$ converges
%to the solution in finite time if \eqref{PL} is a Linear Programming problem ($Q=0$) or a strictly convex Quadratic problem ($Q > 0$). For this purpose we need some
%assumptions based on the concepts described as follows.

\begin{defi} \label{hipregular} Given $x \in \R^n$, let $I^\ell_x$ and $I^u_x$
	be matrices whose columns are the canonical vectors $e_i \in \R^n$
	such that $x_i=\ell_i$ and $x_i=u_i$, respectively. The point $x$ is
	said to be regular for problem \eqref{PL} if the columns of the
	matrix   $[A^T \,\,\, -I^\ell_x \,\,\, I^u_x]$ are linearly independent. Hence, a regular stationary point implies that there are unique $\lambda\in\R^m,\mu_{\ell}\in\R^n_+$ and $\mu_u\in\R^n_+$ that satisfy $$Qx+c+A^T \lambda- \mu_{\ell}+\mu_u=0,$$ with $[\mu_{\ell}]_i=0$ if $x_i>\ell_i$ and $[\mu_u]_i=0$ if $x_i<u_i$. A stationary point such that $[\mu_{\ell}]_i>0$ for all $i$ such that $x_i=\ell_i$, and $[\mu_u]_i> 0$ for all $i$ such that $x_i=u_i$ is said to satisfy the strict complementarity condition.
\end{defi}



\begin{hip}\label{hipregcompestrita}
	The sequence $\{x^k\}$ 	converges to $x^*$, which is a regular solution of  problem \eqref{PL}  that satisfies strict complementarity with $\mu_{\ell}^* <\mu_{\max}$ and $\mu_u^* < \mu_{\max}$.
\end{hip}

The convergence of $\{x^k\}$  follows if the original problem \eqref{PL} has an unique solution. In the case of strictly convex quadratic problems this is always true. Assumption \ref{hipregcompestrita} is usual to demonstrate stronger convergence properties, such as the finite convergence, of linear and nonlinear programming algorithms. Next hypothesis is also  commonly used to demonstrate strong properties of an algorithm when solving an LP problem.


\begin{hip}\label{hipnondegene}
	A basic solution of the LP problem $x^*$ is said to be a non-degenerate solution if $|\alpha_u(x^*)|+|\alpha_{\ell}(x^*)|=n-m$.
\end{hip}


\begin{hip}\label{hipsubprobresolvido}
	The point $x^{k+1}$,  with corresponding approximate Lagrange multiplier $\lambda^{k+1}$, an $\varepsilon_{k+1}$ approximate solution of the subproblem \eqref{subprobLAk}, is such that
	$$\|\nabla L(x^{k+1},\rho_k,\mu_{\ell}^k,\mu_u^k)+ A^T \lambda^{k+1} \| \leq \nu_{k+1} \left\|\max\left\{-\frac{\mu_{\ell}^k}{\rho_k},-\frac{\mu_u^k}{\rho_k},x^{k+1}-u,\ell-x^{k+1}\right\}\right\|,$$
	where $\nu_{k} \to 0$.
\end{hip}


Note that Assumption \ref{hipsubprobresolvido} would be satisfied if the subproblem is solved exactly. Theorem \ref{convfinitasubprob} shows sufficient conditions to ensure this   and it is closely connected with the fact that the penalty parameter is bounded and the strict complementarity assumption.

\begin{lemma}\label{teorholimitado}
	Let $\{\rho^k\}$, be generated bu Algorithm \ref{ALalg} when solving problem \eqref{PL} with $Q=0$ and using the first order update of the Lagrange multipliers. If Assumptions \ref{hipregcompestrita}, \ref{hipnondegene} and \ref{hipsubprobresolvido} holds, then there  exists  $\bar{\rho}$ such that $\rho^k \leq \bar{\rho}$ for all $k$.
\end{lemma}

\noindent {\it Proof.} 
Consider the following system whit $n+m+|\alpha_u(x^*)|+|\alpha_{\ell}(x^*)|$ variables and equations
\begin{equation}\label{sistKKTcompcertas}
Qx+c+A^T \lambda- \mu_{\ell}+\mu_u=0, \,\, Ax=0, \,\, x_{\alpha_u(x^*)}=u_{\alpha_u(x^*)}, \, \text{ and } \, x_{\alpha_{\ell}(x^*)}=u_{\alpha_{\ell}(x^*)}.
\end{equation}
Under Assumption \ref{hipregcompestrita} and \ref{hipnondegene}, if $Q = 0$, we   have that the Jacobian of \eqref{sistKKTcompcertas} is nonsingular. This means that Assumption 7.4 of \cite{bmbook} holds. Thus all the hypotheses of Theorem 7.2 of \cite{bmbook} are satisfied and so it   guarantees the existence of $\bar{\rho}$.  \halmos
 
Next result shows that, using the first order update,  for $k$ large enough, the algorithm identifies correctly the active box constraints indexes.

\begin{lemma} \label{LemaCompcertas} Let $\{x^k\}$, $\{\mu^{k}_{\ell}\}$, and $\{\mu^{k}_u\}$ be sequences generated by Algorithm \ref{ALalg} when solving problem \eqref{PL} with $Q=0$ and using the first order update of the Lagrange multipliers. Suppose that  Assumptions \ref{hipregcompestrita}, \ref{hipnondegene} and \ref{hipsubprobresolvido} hold, then there
exists $k_0 \in \N$ such that, for all $j \geq k \geq k_0$, $\alpha_{\ell}^{k}(x^j) =\alpha_{\ell} (x^*)$ and $\alpha_u^k(x^j)=\alpha_u (x^*)$.
\end{lemma}

\noindent {\it Proof.}  Once again we have that the hypotheses of Lemmas 7.1 and 7.2 and Theorem 7.2 of \cite{bmbook} holds. Therefore, $\lim_{k \to \infty} \mu^k_{\ell}=\mu^*_{\ell}$, $\lim_{k \to \infty} \mu^k_u=\mu^*_u$, $\mu^{k}_{\ell}>0 \Leftrightarrow
\mu^*_{\ell}>0$, $\mu^{k}_u > 0 \Leftrightarrow \mu^*_u>0$, and $\rho_k = \bar{\rho}$ for $k$ large enough. In this case, by the strictly complementarity, we have that for $k$ large enough  $\frac{[\mu^{k}_{\ell}]_i}{\rho_{k}}$ and $\frac{[\mu^{k}_u]_i}{\rho_{k}}$ are bounded away from zero if $x_i^*=\ell_i$ or  $x_i^*=u_i$. Since  $\{x^j\}$ 
converges to $x^*$, we have that $x^{j}_i<\ell_i+\frac{[\mu^{k}_{\ell}]_i}{\rho_{k}}\Leftrightarrow
x^*_i=\ell_i$ and
$x^{j}_i>u_i-\frac{[\mu^{k}_u]_i}{\rho_{k}}\Leftrightarrow
x^*_i=u_i$ for $k$ and $j$ large enough. \halmos

  Next lemma show that the exact Lagrange multipliers are obtained when dealing with Linear Programming problems. By Lemma \ref{LemaCompcertas}, the result could be obtained following directly from Proposition 5.2.1 of \cite{bertsekasCOA} considering the equality constrained subproblem fixing $x_{\alpha_{\ell}(x^*)}=\ell_{\alpha_{\ell}(x^*)}$ and $x_{\alpha_u(x^*)}=u_{\alpha_u(x^*)}$ and penalizing theses constraints. However, we chose to present a new proof because it is more direct and constructive, without the use of the concepts of proximal method. Note that with the strict complementarity hypothesis, the boundedness of the penalty parameter and the identification of the active constraints, the hypotheses of Theorem~\ref{convfinitasubprob} are satisfied and so it is natural to assume that the subproblems are solved exactly for $k$ sufficiently large.

\begin{lemma} \label{convfinitaLAPL} Let $k_0$ be defined as in Lemma \ref{LemaCompcertas} and $\{x^k\}$, $\{\mu^{k}_{\ell}\}$, and $\{\mu^{k}_u\}$ be sequences generated by Algorithm \ref{ALalg} when solving problem \eqref{PL} with $Q=0$ and using the first order update of the Lagrange multipliers. Suppose that  Assumptions \ref{hipregcompestrita} and \ref{hipsubprobresolvido} hold and that subproblems \eqref{subprobLAk} are solved exactly for $k \geq k_0$  ($\varepsilon_k=0$), then   there exists $k_1$ such that $\mu^{k_1}_{\ell}=\mu_{\ell}^*$ and  $\mu^{k_1}_u=\mu_u^*$.
\end{lemma}


\noindent {\it Proof.}  
By Lemma \ref{LemaCompcertas} we have that there exists $k_0 \in K$ such that, for all $k\geq k_0$, 
\begin{equation} \label{compcertasl}
\left[\max\left\{\ell-x^{k_0}+\frac{\mu^{k_0}_{\ell}}{\rho_{k_0}},0\right\}\right]_i= \left\{ 
\begin{array}{c}
\left[\ell-x^{k_0}+\frac{\mu^{k_0}_{\ell}}{\rho_{k_0}}\right]_i, \text{ if } x^*_i=\ell_i\\ 
0, \text{otherwise}
\end{array}
\right.
\end{equation}
and
\begin{equation} \label{compcertasu}
\left[\max\left\{x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}},0\right\}\right]_i= \left\{ 
\begin{array}{c}
\left[x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}}\right]_i, \text{ if } x^*_i=u_i\\ 
0, \text{otherwise}
\end{array}
\right. 
\end{equation}

Since problem \eqref{subprobLAk} is linearly constrained, there exists $\lambda^{k_0} \in \R^m$ such
that $(x^{k_0},\lambda^{k_0})$ satisfies the system
\begin{equation} \label{KKTemK0}
c+A^T \lambda^{k_0}- \rho_{k_0}\max\left\{\ell-x^{k_0}+\frac{\mu^{k_0}_{\ell}}{\rho_{k_0}},0\right\}+
\rho_{k_0}\max\left\{x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}},0\right\}=0. 
\end{equation}


Since $x^*$ is a KKT regular point for \eqref{PL}, there exists $(\lambda^*,\bar{\mu}_{\ell}^*,\bar{\mu}_u^*)$ 
unique solution for the system 
\begin{equation}\label{multiplunicos}
[A^T \,\,\, -I^\ell_{x^*} \,\,\, I^u_{x^*}] \left(\begin{array}{c}
\lambda\\
\mu_{\ell}\\
\mu_u
\end{array}\right)
=-c,
\end{equation}
and $[\mu_{\ell}^*]_i=[\bar{\mu_{\ell}}^*]_i$ if $x^*_i=\ell_i$ and  $[\mu_{\ell}^*]_i=0$ otherwise and 
$[\mu_u^*]_i=[\bar{\mu_u}^*]_i$ if $x^*_i=u_i$ and  $[\mu_u^*]_i=0$ otherwise.

So, we have that $\lambda^{k_0}=\lambda^*$ and, by \eqref{compcertasl}, \eqref{compcertasu} and \eqref{KKTemK0},
$\rho_{k_0}\max\left\{\ell-x^{k_0}+\frac{\mu^{k_0}_{\ell}}{\rho_{k_0}},0\right\}=\mu_{\ell}^*$ and 
$\rho_{k_0}\max\left\{x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}},0\right\}=\mu_u^*$.


By \eqref{compcertasl}, \eqref{compcertasu} and the definition of first order update rule for the Lagrange multipliers we
have that
$$\mu_{\ell}^{k_0+1}=\mu_{\ell}^*$$
and
$$\mu_u^{k_0+1}=\mu_u^*.$$ \halmos




We are now ready to prove the finite convergence of the method for LP problems.

\begin{theorem} \label{convfinitaLA2} Let $k_0$ be defined as in Lemma \ref{LemaCompcertas} and $\{x^k\}$ be   generated by Algorithm \ref{ALalg} when solving problem \eqref{PL} with $Q=0$ and using the first order update of the Lagrange multipliers. Suppose that  Assumptions \ref{hipregcompestrita} and \ref{hipsubprobresolvido} hold and that subproblems \eqref{subprobLAk} are solved exactly for $k \geq k_0$, then there exists $k_2$ such 	that $x^{k_2}=x^*$.
\end{theorem}


\noindent {\it Proof.}  
Let $k_1$ be defined as in Lemma \ref{convfinitaLAPL} such that  $\mu^{k_1}_{\ell}=\mu_{\ell}^*$ and  $\mu^{k_1}_u=\mu_u^*$. Since $x^{k_1+1}$ is a solution of the linearly constrained problem \eqref{subprobLAk}, there exists $\lambda^{k_1+1} \in \R^m$ such
that $(x^{k_1+1},\lambda^{k_1+1})$ satisfies the system
\begin{equation} \label{KKTemK0mais1}
\begin{array}{c}
c+A^T \lambda^{k_1+1}- \rho_{k_1}\max\left\{\ell-x^{k_1+1}+\frac{\mu^*_{\ell}}{\rho_{k_1}},0\right\}+
\rho_{k_1}\max\left\{x^{k_1+1}-u+\frac{\mu^*_u}{\rho_{k_1}},0\right\}=0, \\[5pt]
A x^{k_1+1}=b.
\end{array}
\end{equation}

By the uniqueness of the solution of \eqref{multiplunicos} and Lemma \eqref{LemaCompcertas} we have that 
$x^{k_1+1}_i=\ell_i \Leftrightarrow x^*_i=\ell_i$ and $x^{k_1+1}_i=u_i \Leftrightarrow x^*_i=u_i$. So
$x^{k_1+1}$ is a KKT point for \eqref{PL} and thus it is a solution of \eqref{PL}.
\halmos






















We now turn our attention to discuss the  finite convergence results for strictly convex problems. The use of Assumption \ref{hipnondegene} does not make sense in this context, since it is not defined the concept of basic solutions to quadratic problems. However, the only use of this hypothesis was to ensure that the Jacobian of \eqref{sistKKTcompcertas} is nonsingular. With Assumption \ref{hipregcompestrita}, the fact that $Q$ is positive definite  already guarantees this and therefore Assumption \ref{hipnondegene} is not relevant. This means that, under Assumptions \ref{hipregcompestrita} and \ref{hipsubprobresolvido}, the thesis of Theorem 7.2 of \cite{bmbook} also holds for the strictly convex problems using the first order update rule to define the Lagrange multipliers estimators. The similarity of this algorithm with the one that uses the hybrid Lagrange multipliers update justifies the plausibility of the boundedness of the penalty parameter until we got that $\alpha_{\ell}^{k}(x^k) =\alpha_{\ell} (x^*)$ and $\alpha_u^k(x^k)=\alpha_u (x^*)$.


%\begin{hip}\label{hiprholimitado}
%	There exists  $\bar{\rho}$ such that $\rho^k \leq \bar{\rho}$ for all $k$.
%\end{hip}

Furthermore, the hypotheses of Lemmas 7.1 and 7.2  of \cite{bmbook} holds whit the first order update rule. This means that, under Assumptions \ref{hipregcompestrita} and \ref{hipsubprobresolvido}, the algorithm method with the first-order update generates sequences that converge to exact multipliers. 
Thus, in the same way as in Lemma \ref{LemaCompcertas}, using the strictly complementarity and the boundedness of $\rho_k$, we can prove that the algorithm applying the first order update to strictly convex problems correctly identifies the active constraints for $k$ sufficiently large. This suggests that plausibility of Assumption \ref{hipregcerta} for the sequences generated by the hybrid method.

\begin{hip}\label{hipregcerta}
	There 	exists $k_0 \in \N$ such that, for all $j \geq k \geq k_0$, $\alpha_{\ell}^{k}(x^j) =\alpha_{\ell} (x^*)$ and $\alpha_u^k(x^j)=\alpha_u (x^*)$.
\end{hip}

 As in the case of LP problems, this means that, under Assumption \ref{hipregcompestrita}, Theorem \ref{convfinitasubprob} ensures that the subproblems are solved exactly for $k \geq k_0$. Thus, we can also demonstrate the finite convergence of the multipliers in the strictly convex case as follows.

\begin{lemma} \label{convfinitaLAQP} Consider that problem \eqref{PL} is defined with $Q$ positive definite and that $\{x^k\}$, $\{\mu^{k}_{\ell}\}$, and $\{\mu^{k}_u\}$ are sequences generated by Algorithm \ref{ALalg} using the hybrid Lagrange multipliers update when solving it. Suppose that  Assumptions \ref{hipregcompestrita} and \ref{hipregcerta} hold and that subproblems \eqref{subprobLAk} are solved exactly for $k \geq k_0$, $k_0$ defined at Assumption \ref{hipregcerta}. Then, there exists $k_1 \geq k_0$ such 	that $\mu^{k_1}_{\ell}=\mu_{\ell}^*$ and $\mu^{k_1}_u=\mu_u^*$.
\end{lemma}

\noindent {\it Proof.}  
Note that because the subproblem is piecewise quadratic  and Newton's method uses only local information, the trial point $x^{k+1}_{trial}$ is the solution of the smooth quadratic problem
\begin{equation}\label{subprobsuave}
 \min_x L^k(x, \mu^k) \text{ s.t. } Ax=b,
 \end{equation}
where
$$L^k(x, \mu) = \frac{1}{2}x^TQx+c^Tx+\sum_{i \in \alpha_{\ell}^k(x^k) } \left(\ell_i-x_i+\frac{[\mu_{\ell}]_i}{\rho_k}\right)^2 +\sum_{i \in \alpha_u^k(x^k) } \left( x_i-u_i+\frac{[\mu_u]_i}{\rho_k}\right)^2.$$

Therefore, the first order optimality conditions of \eqref{subprobsuave} ensures that $\nabla L^k(x^{k+1}_{trial},\mu^k)^Td=0$ for all $d$ such that $Ad=0$. 
Defining the smooth unidimensional quadratic $g_k(t)=L^k(x^k+t(x^{k+1}_{trial}-x^k),  \mu^k)$, since $d=x^{k+1}_{trial}-x^k$ lies in the Kernel of $A$,  by the chain rule, we have that $g_k'(1)=0$.



On the other hand, $x^{k+1}=x^k+t^k(x^{k+1}_{trial}-x^k)$, where $t^k$ is the solution of
$$\min \, \bar{g}_k(t)=L(x^k+t(x^{k+1}_{trial}-x^k),\rho_k,\mu_{\ell}^k,\mu_u^k).$$
Since $\bar{g}_k(t)$ has first order derivatives we have that $\bar{g}_k'(t^k)=0$. Using the chain rule once again, we got that    $\nabla L(x^{k+1},\rho_k,\mu_{\ell}^k,\mu_u^k)^T(x^{k+1}_{trial}-x^k)=0$. If  $k_0$ is such that $\alpha_{\ell}^{k_0}(x^{{k_0}+1})=\alpha_{\ell}^{{k_0}}(x^{k_0})$ and $\alpha_u^{k_0}(x^{{k_0}+1})=\alpha_u^{{k_0}}(x^{k_0})$ then $\nabla L(x^{{k_0}+1},\rho_{k_0},\mu_{\ell}^{k_0},\mu_u^{k_0})= \nabla L^{k_0}(x^{{k_0}+1}, \mu^{k_0})$ and so  $g'_{k_0}(t^{k_0})=0$. Since $g_{k_0}(t)$ is a strictly convex function, its minimizer is unique and therefore $t^{k_0}=1$, which means that $x^{{k_0}+1}=x^{{k_0}+1}_{trial}$.


 Using the approach of \cite{yaxyuan}, note that by Assumption \ref{hipregcerta}, the second-order update consists of applying Newton's method to obtain $x(\mu)_{\alpha_{\ell}(x^*)} = l_{\alpha_{\ell}(x^*)}$ and $x(\mu)_{\alpha_u(x^*)} = u_{\alpha_u(x^*)}$, where $x(\mu)$ is, for a fixed $\rho_k$, the unique  solution of \eqref{subprobLA}  given $\mu_{\ell}$ and $\mu_u$. Since $x^{{k_0}+1}=x^{{k_0}+1}_{trial}$ we have that $x(\mu^{k_0})=x^{{k_0}+1}$. Since $x(\mu)$ is linear, the Newton's method finds the exact solution in a single iteration. This means that $(\mu_{\ell}^{{k_0}+1},\mu_u^{{k_0}+1})=(\mu_{\ell}^*,\mu_u^*)$.
\halmos

Once the exact Lagrange multipliers are obtained, the primal solution is also found.

\begin{theorem} \label{convfinitaLAQP2} Consider that problem \eqref{PL} is defined with $Q$ positive definite and that $\{x^k\}$ is generated by Algorithm \ref{ALalg} using the hybrid Lagrange multipliers update when solving it. Suppose that  Assumptions \ref{hipregcompestrita} and \ref{hipregcerta} hold and that subproblems \eqref{subprobLAk} are solved exactly for $k \geq k_0$, $k_0$ defined at Assumption \ref{hipregcerta}. Then, there exists $k_2$ such 	that $x^{k_2}=x^*$.
\end{theorem}


\noindent {\it Proof.}  
By Lemma \ref{convfinitaLAQP}, there exists $k_1 \geq k_0$ such that  $\mu^{k_1}_{\ell}=\mu_{\ell}^*$ and $\mu^{k_1}_u=\mu_u^*$. Therefore $x^{k_1+1}$ is a KKT point  of the subproblem and so  there exists $\lambda^{k_1+1} \in \R^m$ such that $(x^{k_1+1},\lambda^{k_1+1})$ satisfies the system
\begin{equation} \label{KKTQPemK0mais1}
\begin{array}{c}
Qx^{k_1+1}+c+A^T \lambda^{k_1+1}- \rho_{k_1}\max\left\{\ell-x^{k_1+1}+\frac{\mu^*_{\ell}}{\rho_{k_1}},0\right\}+
\rho_{k_1}\max\left\{x^{k_1+1}-u+\frac{\mu^*_u}{\rho_{k_1}},0\right\}=0, \\[5pt]
A x^{k_1+1}=b.
\end{array}
\end{equation}
Since $(x^*, \lambda^*)$ is also solution of system \eqref{KKTQPemK0mais1} and the subproblem is strictly convex, we must have that $(x^{k_1+1},\lambda^{k_1+1})=(x^*, \lambda^*)$.
\halmos







\section{Numerical experiments}
\graphicspath{ {./figures/} }

The goal of our numerical experiments is to show that the augmented Lagrangian method proposed is compatible with a simple implementation of a pure interior point method. This is done in order to motivate further studies on augmented Lagrangian methods applied to linear and convex problems. We implemented Algorithm \ref{ALalg} (called \texttt{LAQP}) and the interior point method for convex quadratic programming as described in~\cite{gondzio25} (called \texttt{IPM}), both in Julia~\cite{Bezanson:2017g}. In \texttt{IPM}, finite upper bounds were included as new constraints by adding slack variables. We used subroutine MA57~\cite{Duff:2004cx} from HSL~\cite{HSL:7NBcL3Rm} to solve the augmented linear systems that arise in both methods. We run the tests on an iMac Pro with Intel Xeon W 3.2 GHz Processor and 32 GB RAM.
 
Our tests are based on the \textsc{Netlib}~\cite{Dongarra:1987jk} test collection of linear programming problems, which consists of $98$ problems where, as reported in~\cite{Haeser:2017wl}, circa $65\%$ of them do not have strict interior, which indicates that those  are rather difficult problems. Hence, we used a presolved version of the collection available at~\cite{HagerCoapSoftware}. However, in our tests on linear programming problems, we found out that our implementation is too sensible to a fine tuning of the regularization parameter and the results were not satisfactory. Thus, we added a homogeneous quadratic term to the objective function with Hessian equal to the identity matrix, in order to obtain strictly convex separable quadratic problems. In this way, we may take the regularization parameter equal to zero at each iteration and we may use the second-order Lagrange multiplier update, which is easily computable. Also, the Newtonian direction can be accurately computed without refined linear solvers.

We set the initial point using two steps. In the first step, we choose one of the four possible initial point candidates:

\begin{description}
  \item[\texttt{New}:] in this approach, $x^0$ is computed as the projection of the solution of  $\min \frac{1}{2}x^{T}Qx + c^Tx $, subject to $Ax=b$, onto the box constraints. That is, a solution of \eqref{PL} ignoring the box constraints is computed and then projected onto the box constraints. Next, $\lambda^{0}$ is computed as the least squares solution of $A^{T}\lambda = -(Qx^{0}+c)$, while  $\mu_{\ell}^0=-\min\{0, \mu\}$  and
$\mu_u^0=\max\{0,\mu\}$, with $\mu = -(Qx^{0}+c+A^{T}\lambda^{0})$;


\item [\texttt{Ones}:] we define $x^{0}$ and $\mu_{\ell}^0$ as the vector of ones and $\lambda^{0}$ and $\mu_u^0$ as the vector of zeros;

\item [\texttt{Mehrotra}:] $x^{0}$, $\lambda^{0}$, $\mu_{\ell}^0$ and $\mu_u^0$ are computed by the usual Mehrotra-type heuristic~\cite{Mehrotra:1992wr};

\item [\texttt{OnesM}:] we define $x^{0}$ as the vector of ones, $\mu_{\ell}^0 = Qx^{0}+c +\beta I$, where $\beta\geq0$ is chosen such that all components of $\mu_{\ell}^0$ are at least $\num{0.1}$,  $\lambda^{0}$ is computed as the least squares solution of $A^{T}\lambda = -(Qx^{0}+c-\mu_{\ell}^0)$,  and $\mu_u^0$ is the vector of zeros.


\end{description}

Independently of the choice above, the second step depends on the method: for \texttt{LAQP}, we perform a full Newton step to recover feasibility regarding $Ax=b$; for  \texttt{IPM}, we use the shifting strategy implemented by Mehrotra~\cite{Mehrotra:1992wr} to assure that the initial point is (sufficiently) interior with respect to the box constraints. 

We start by considering $62$ (out of $95$\footnote{Problems \texttt{forplan, gfrd-pnc} and \texttt{pilot.we} were removed from the \textsc{Netlib} collection, since they were not available in the presolved library \cite{HagerCoapSoftware}.}) \textsc{Netlib} problems with less than \num{10000} non-zero elements in the matrix of constraints in order to decide  which of those aforementioned four possible initial points provide better numerical results, for each method.

With respect to the choice of Lagrange multiplier update for \texttt{LAQP}, we use a \texttt{Hybrid} approach, which uses the second-order update only when the solution of the subproblem is within the same region as the current iterate, in the sense that the corresponding Hessians of the augmented Lagrangian function coincide. Otherwise, the first-order update is used. The performance profiles, computed in terms of the number of linear systems solved, are presented in \Cref{fig:PP-4points}, where for each algorithm all initial points are compared.

%All initial points LAQP and IPM on the SMALL subset of \textsc{Netlib} (with Hybrid update?).
\begin{figure}[!ht]\centering
\begin{subfigure}[t]{.72\textwidth}
\centering
\includegraphics[width=\textwidth]{PP_IPM-4points}
   \caption{}
  \label{fig:PP_IPM-4points}
  
\end{subfigure} 
\\
   % \hspace*{.21in}  
   \begin{subfigure}[t]{.72\textwidth}
\centering
\includegraphics[width=\textwidth]{PP_LAQPHybrid-4points}
   \caption{}
  \label{fig:PP_LAQPHybrid-4points}

\end{subfigure}

\caption{Performance profiles of \texttt{IPM} and \texttt{LAQP-Hybrid} with each of the four initial points.}\label{fig:PP-4points}
\end{figure}

Based on these results, we decided to use \texttt{Mehrotra}  as the initial point for \texttt{IPM} and \texttt{New} for \texttt{LAQP}. We now verify, on \Cref{fig:PP_LAQP_Dual-Linear-Hybrid}, the efficacy of the \texttt{Hybrid} strategy for updating the Lagrange multipliers. We compare it with the \texttt{Linear} update, where the first-order update is used in every iteration, and the \texttt{Dual} update, where the second-order update is always used.
\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{PP_LAQP_Dual-Linear-Hybrid}
\caption{Performance profile for  \texttt{LAQPHybrid-New}, \texttt{LAQPLinear-New} and   \texttt{LAQPDual-New}.}
\label{fig:PP_LAQP_Dual-Linear-Hybrid}
\end{figure}
Based on these results, we conclude that the augmented Lagrangian method with the \texttt{Hybrid} Lagrange multiplier update and the \texttt{New} initial point had the best performance in our tests.

We now compare our best version of \texttt{LAQP}, that is, \texttt{LAQPHybrid-New} versus \texttt{IPM-Mehrotra}. The performance profile is presented in \Cref{fig:PP_LAQPHybridNew-IPMMehrotra}. \texttt{IPM} was clearly superior in this test as it solved $57$ problems ($92\%$) while \texttt{LAQP} solved $49$ problems ($79\%$). Also, \texttt{IPM} was faster in $41$ problems ($66$\%), \texttt{LAQP} was faster in $19$ problems ($31$\%) while $2$ problems where not solved by any method. We present these results in more details in a more thorough test on all $95$ \textsc{Netlib} problems, modified in the same way as describe before. The corresponding performance profile is given in \Cref{fig:PP_LAQPNew-IPMMehrotra_All}.

%The same test on the SMALL subset{}

\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{PP_LAQPHybridNew-IPMMehrotra}
\caption{Performance profile of \texttt{IPM-Mehrotra} versus \texttt{LAQPHybrid-New}.}\label{fig:PP_LAQPHybridNew-IPMMehrotra}
\end{figure}

%NEW initial point LAQP vs MEHROTRA IPM on all \textsc{Netlib}
\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{PP_LAQPNew-IPMMehrotra_All}
\caption{Performance profile of \texttt{IPM-Mehrotra} versus \texttt{LAQPHybrid-New} for all 95 \textsc{Netlib} problems.}\label{fig:PP_LAQPNew-IPMMehrotra_All}
\end{figure}


%We emphasize that our goal is to compare a pure interior point method versus the pure augmented Lagrangian method proposed, without sophisticated accelerations and safeguards. This explains the overall poor behavior of both methods.  and, in particular, explains the overall poor behavior of the pure interior point implementation.


To analyze the results in this larger test, in \Cref{fig:Bar_LAQPNew-IPMMehrotra_All_2}, we present a ratio calculated for each of the $95$ available problems on the horizontal axis, ordered in increasing size of non-zero entries in the constraint matrix. A bar with a positive vertical coordinate indicates that \texttt{LAQP} is the best solver in terms of the number of linear systems solved, while a negative coordinate indicates that \texttt{IPM} is better. The size of the bar corresponds to the percentage difference in the number of linear systems solved by both solvers with respect to the worst one and it is given by the absolute value of $r_i$, for each problem $i$, computed as 
\[
r_i = \frac{\#S_{\mathtt{IPM}}^{i} - \#S_{\mathtt{LAQP}}^{i} }
{\max\{ \#S_{\mathtt{IPM}}^{i} , \#S_{\mathtt{LAQP}}^{i} \}},
\] where $\#S_{j}^{i}$ is the number of linear systems used by solver $j$ to solve problem $i$. For instance, if $r_i = 0.8$ this means that \texttt{LAQP} reduced in 80\% the number of linear systems solved with respect to \texttt{IPM}. If only one solver found a solution, $r_i$ is either $1.0$ or $-1.0$, and a red cross is marked when none of the solvers found a solution.  %solved $5$ times more linear systems than \texttt{LAQP} on problem $i$ and the bar points upwards, while $r_i = -0.3$ indicates that \texttt{IPM} was 30\% better on problem $i$, i.e., \texttt{LAQP} solved $10/7$ times more linear systems than \texttt{IPM} and the bar points downwards. 

We can see that in these added $33$ largest problems of the \textsc{Netlib} collection, \texttt{LAQP} failed in less problems: both methods failed in $12$ problems, \texttt{IPM} failed in $10$ problems solved by \texttt{LAQP} while \texttt{LAQP} failed in $5$ problems solved by \texttt{IPM}. That is, \texttt{IPM} failed in $67\%$ of the largest problems while \texttt{LAQP} failed in $52\%$.

% BAR graphic
\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{Bar_LAQPNew-IPMMehrotra_All_2}
\caption{Ratio of performance between \texttt{LAQPHybrid-New} and \texttt{IPM-Mehrotra}.}
\label{fig:Bar_LAQPNew-IPMMehrotra_All_2}
\end{figure}

%with the NEW initial point on LAQP, check that the second-order update is indeed better than the first-order one

We envision that the main quality of our method, in comparison with interior point methods, is the possibility of naturally exploiting a very good initial point, without the need of projecting onto a strict interior. Hence, we consider the previous test of \texttt{LAQPHybrid-New} and \texttt{IPM-Mehrotra} on the $62$ smallest \textsc{Netlib} (modified) problems and we select only the $46$ problems where both solvers were successful. We then take a perturbation of the solution $(x^*,\lambda^*,\mu_{\ell}^*,\mu_u^*)$ found by each solver and we run each solver again with this point as the initial point. The perturbation for the augmented Lagrangian is given by $[x^0]_i=(1+10^{-3}n_i)[x^*]_i+m_i$, where $n_i$ is sampled from the standard normal distribution and $m_i$ is a random value between $-10^{-5}$ and $10^{-5}$, for each coordinate $i$. A similar perturbation is performed for $\lambda^0, \mu_{\ell}^0$ and $\mu_u^0$. For the interior point method, after the perturbation of $(x^*,\lambda^*,\mu_{\ell}^*,\mu_u^*)$ as described above, we perform a Mehrotra type shift to obtain an interior initial point and its corresponding multipliers.  The \texttt{LAQPHybrid} variant of \texttt{LAQP} always performs the first Lagrange multiplier update as the first-order one, which we have observed that it is not efficient for solving the problem from a perturbed solution. Hence, in this test, we always perform the second-order update, that is, \texttt{LAQPDual} is run when resolving the problem.

The results are on \Cref{tab:LAQP_IPM_Perturbed_x} where the first column is the name of the problem on the \textsc{Netlib} collection, the second column is the number of non-zero entries of the matrix $A$, the third and forth columns are the number of iterations of \texttt{LAQP} and \texttt{IPM}, respectively, needed for solving the problem originally from their corresponding initial points, while the fifth and sixth columns correspond to the number of additional iterations for \texttt{LAQP} and \texttt{IPM}, respectively, needed for solving the problem from a perturbation of the solution. We note that \texttt{LAQP} was not able to solve three problems from the perturbed solution, marked with a dash on the table.

\begin{table}[!hbt]\centering
\caption{Number of linear systems solved before and after starting from a perturbed solution for \texttt{LAQP} and \texttt{IPM}.}
\label{tab:LAQP_IPM_Perturbed_x}\small
\begin{tabular}{>{\ttfamily}lS[table-format=5,group-minimum-digits=3]cccc}
\toprule
    & &  \multicolumn{2}{c}{First run} &   \multicolumn{2}{c}{Second run}  \\
{\normalfont \textbf{Problem}}  & {\textbf{Nonzeros}} &\texttt{LAQP} &   \texttt{IPM} & \texttt{LAQP}  & \texttt{IPM} \\ 
\cmidrule(r){1-1}  
\cmidrule(lr){2-2}  
\cmidrule(lr){3-4}  
 \cmidrule(l){5-6}
afiro    & 88    &   16   &  \textbf{13}   &     \textbf{2}   &    3 \\ 
adlittle & 465   &   61   &  \textbf{17}   &     \textbf{2}   &    5 \\ 
agg2     & 4515  &   64   &  \textbf{34}   &     \textbf{2}   &    7 \\ 
agg3     & 4531  &   61   &  \textbf{33}   &     \textbf{2}   &    6 \\ 
bandm    & 2659  &  405   & \textbf{146}   &   148   &    \textbf{8} \\ 
beaconfd & 3476  &    \textbf{2}   &  15   &     \textbf{2}   &    5 \\ 
blend    & 521   &   22   &  \textbf{16}   &     \textbf{2}   &    4 \\ 
bore3d   & 1525  &   \textbf{36}   &  38   &    20   &    \textbf{7} \\ 
brandy   & 2150  &   \textbf{91}   & 167   &   --    &    \textbf{6} \\ 
capri    & 1786  &  \textbf{237}   & 291   &    70   &   \textbf{10} \\ 
e226     & 2767  &   73   &  \textbf{28}   &    77   &    \textbf{6} \\ 
ganges   & 7021  &  152   &  \textbf{28}   &    24   &    \textbf{8} \\ 
grow15   & 5665  &   \textbf{76}   &  79   &     \textbf{2}   &   43 \\ 
grow22   & 8318  &   \textbf{40}   &  84   &     \textbf{2}   &   48 \\ 
grow7    & 2633  &   \textbf{71}   &  80   &     \textbf{5}   &   44 \\ 
israel   & 2358  &  179   & \textbf{132}   &   115   &    \textbf{9} \\ 
kb2      & 291   &  108   &  \textbf{20}   &    89   &    \textbf{6} \\ 
lotfi    & 1086  &   \textbf{14}   &  17   &     \textbf{2}   &    5 \\ 
modszk1  & 4158  &  294   &  \textbf{50}   &   --    &    \textbf{6} \\ 
qap8     & 8304  &  131   &  \textbf{16}   &   --    &    \textbf{4} \\ 
recipe   & 752   &    \textbf{2}   &  31   &    \textbf{7}   &   15 \\ 
sc105    & 281   &    \textbf{8}   &  21   &     \textbf{2}   &    3 \\ 
sc205    & 552   &   \textbf{16}   &  62   &     \textbf{2}   &    4 \\ 
sc50a    & 131   &    \textbf{2}   &   7   &     \textbf{2}   &    3 \\ 
sc50b    & 119   &    \textbf{4}   &  10   &     \textbf{2}   &    3 \\ 
scagr25  & 2029  &   73   &  \textbf{34}   &     6   &    6 \\ 
scagr7   & 553   &   52   &  \textbf{19}   &     \textbf{2}   &    4 \\ 
scfxm1   & 2612  &  122   &  \textbf{76}   &    27   &    \textbf{8} \\ 
scfxm2   & 5229  &  207   & \textbf{193}   &    86   &    \textbf{8} \\ 
scorpion & 1708  &  102   &  \textbf{28}   &    45   &    \textbf{8} \\ 
scrs8    & 4029  &  195   &  \textbf{31}   &   195   &    \textbf{6} \\ 
scsd1    & 3148  &   \textbf{42}   &  93   &    56   &   \textbf{47} \\ 
scsd6    & 5666  &   \textbf{58}   &  92   &     \textbf{5}   &   46 \\ 
sctap1   & 2052  &   69   &  \textbf{45}   &     \textbf{3}   &    7 \\ 
sctap2   & 8124  &  126   &  \textbf{38}   &    30   &    \textbf{7} \\ 
seba     & 4874  &    \textbf{9}   &  12   &     \textbf{2}   &    3 \\ 
share2b  & 730   &   43   &  \textbf{37}   &     \textbf{2}   &   10 \\ 
shell    & 4900  &  131   &  \textbf{36}   &   129   &    \textbf{6} \\ 
ship04s  & 5810  &  150   &  \textbf{26}   &    28   &    \textbf{5} \\ 
stair    & 3857  &   81   &  \textbf{59}   &     \textbf{5}   &    8 \\ 
standata & 3038  &   74   &  \textbf{37}   &     7   &    \textbf{5} \\ 
standgub & 3147  &   74   &  \textbf{37}   &    38   &    \textbf{6} \\ 
standmps & 3686  &  121   &  \textbf{51}   &    33   &   \textbf{17} \\ 
stocfor1 & 474   &   15   &  \textbf{13}   &     5   &    \textbf{4} \\ 
stocfor2 & 9492  &   70   &  \textbf{52}   &    12   &    \textbf{6} \\ 
vtp-base & 4523  &   38   &  \textbf{33}   &     \textbf{2}   &    5  \\ 
\bottomrule
\end{tabular}
\end{table}



In \Cref{tab:LAQP_IPM_Perturbed_x} we can see that in $22$ problems ($48$\%), \texttt{LAQP} found the solution faster than \texttt{IPM} on the second run. In this cases, \texttt{LAQP} took in average $2.7$ additional iterations, while \texttt{IPM} took, in average, $4.5$ iterations, excluding four cases where more than $40$ additional iterations were necessary. In $23$ problems ($50$\%) where \texttt{IPM} found the solution faster than \texttt{LAQP}, it did so taking in average $7$ additional iterations (excluding one case with more than $40$ iterations), while \texttt{LAQP} took clearly a larger number of iterations. We summarize the results as follows: each method was able to recover the solution faster than the other in circa half of the problems. When \texttt{LAQP} was faster, it took $2.7$ additional iterations, while \texttt{IPM} took in average $7$ additional iterations when it was the fastest. \textcolor{red}{Essa frase est estranha But it is clear that when \texttt{IPM} was not the fastest method, it took less additional iteration than when \texttt{LAQP} was not the fastest.}

We consider again this collection of $46$ problems where both methods found a solution and their corresponding solutions found, say, $x^*$. We now perform two tests by using the solutions found as initial points to a perturbed problem. The first test consists of perturbing the problem in the following way: We find the first index $i$ of $x^*$ such that $\ell_i=[x^*]_i$ and we then redefine $\ell_i\leftarrow \ell_i+1$. This corresponds to a sensibility analysis perturbation. Now $x^*$ is infeasible but it can be used as the initial point for \texttt{LAQP}. For \texttt{IPM} we use $x^*$ and the corresponding Lagrange multipliers followed by a Mehrotra shift as the initial point, to recover interiority. The second test consists of finding the index of the largest interior component of $x^*$, that is, $\ell_i<[x^*]_i<u_i$. For this index we re-define $u_i\leftarrow [x^*]_i-1$, which is a cut similar to usually employed ones in branch and bound strategies. Since in our implementation of \texttt{IPM} upper bounds are treated explicitly by adding slack variables, there is no need to recover interiority and we may use $x^*$ and the corresponding Lagrange multipliers as the initial point. In both tests we consider appropriate safeguards to ensure $\ell\leq u$. The results are reported on \Cref{tab:LAQP_IPM_Perturbed_l}, where a dash corresponds to a solver not being able to solve the problem.

\begin{table}[!hbt]\centering
\caption{Number of systems solved after perturbing the problem with respect to an active bound (Test 1) and an inactive bound (Test 2).}
\label{tab:LAQP_IPM_Perturbed_l}\small
\begin{tabular}{>{\ttfamily}lcccc}
\toprule
    &  \multicolumn{2}{c}{Test 1} &   \multicolumn{2}{c}{Test 2}  \\
{\normalfont \textbf{Problem}} & \texttt{LAQP}   & \texttt{IPM} & \texttt{LAQP}   & \texttt{IPM} \\ 
\cmidrule(r){1-1}  
\cmidrule(lr){2-3}  
\cmidrule(lr){4-5}  
 afiro    &     \textbf{7} &    8   &      \textbf{2}  &     6        \\
 adlittle &     \textbf{2} &    5   &      \textbf{2}  &     6        \\
 agg2     &     \textbf{2} &    4   &      \textbf{2}  &     8        \\
 agg3     &     \textbf{2} &    4   &      \textbf{2} &     6        \\
 bandm    &     8 &    \textbf{4}   &    --   &    \textbf{81}        \\
 beaconfd &     \textbf{2} &    3   &      \textbf{2}  &     5        \\
 blend    &   \textbf{137} &  233   &      \textbf{7}  &    10        \\
 bore3d   &    10 &    \textbf{9}   &      \textbf{2}  &     8        \\
 brandy   &     \textbf{4} &    8   &      \textbf{2}  &     6        \\
 capri    &    16 &    \textbf{9}   &      \textbf{2}  &    16        \\
 e226     &   --  &  --   &      \textbf{2}  &     6        \\
 ganges   &    75 &    \textbf{4}   &     35  &     \textbf{6}        \\
 grow15   &     \textbf{5} &   16   &      \textbf{6}  &    39        \\
 grow22   &     \textbf{5} &   24   &      \textbf{9}  &   154        \\
 grow7    &    51 &   \textbf{35}   &      \textbf{4}  &    24        \\
 israel   &    67 &   \textbf{10}   &      \textbf{6}  &    13        \\
 kb2      &    \textbf{42} &  114   &     48  &    \textbf{15}        \\
 lotfi    &     \textbf{2} &    3   &      \textbf{2}  &     5        \\
 modszk1  &   417 &    \textbf{5}   &    --   &     \textbf{6}        \\
 qap8     &   --  &  --   &    --   &   \textbf{138}        \\
 recipe   &    \textbf{28} &   38   &    --   &    \textbf{20}        \\
 sc105    &     \textbf{2} &    4   &      \textbf{2}  &     7        \\
 sc205    &     \textbf{2} &    4   &      \textbf{2}  &     5        \\
 sc50a    &     \textbf{2} &   --   &      \textbf{2}  &     6        \\
 sc50b    &     \textbf{2} &    4   &      \textbf{3}  &     9        \\
 scagr25  &     \textbf{2} &    4   &      \textbf{2}  &     9        \\
 scagr7   &     \textbf{2} &    4   &      \textbf{2}  &     9        \\
 scfxm1   &     \textbf{4} &    9   &      \textbf{7}  &    14        \\
 scfxm2   &    49 &    \textbf{8}   &    --   &    \textbf{25}        \\
 scorpion &    \textbf{36} &  360   &     35  &     \textbf{9}        \\
 scrs8    &     \textbf{2} &  --   &      \textbf{2}  &     6        \\
 scsd1    &    22 &   \textbf{12}   &     \textbf{19}  &    57        \\
 scsd6    &   106 &   \textbf{42}   &    135  &    \textbf{99}        \\
 sctap1   &    \textbf{25} &   59   &      \textbf{3}  &     9        \\
 sctap2   &    \textbf{36} &  206   &      \textbf{6}  &    13        \\
 seba     &     \textbf{2} &    4   &      \textbf{2}  &     8        \\
 share2b  &    \textbf{24} &   80   &     24  &    \textbf{15}        \\
 shell    &    15 &    \textbf{4}   &    --   &    \textbf{22}        \\
 ship04s  &    32 &    \textbf{5}   &      \textbf{5}  &     7        \\
 stair    &    \textbf{11} &   45   &     16  &     \textbf{7}        \\
 standata &   391 &   \textbf{18}   &     34  &    \textbf{29}        \\
 standgub &   391 &   \textbf{18}   &     34  &    \textbf{29}        \\
 standmps &   124 &   \textbf{42}   &    104  &    \textbf{29}        \\
 stocfor1 &    61 &   \textbf{12}   &     24  &    \textbf{20}        \\
 stocfor2 &   \textbf{270} &  274   &    --   &   \textbf{152}        \\
 vtp-base &     \textbf{2} &    4   &      \textbf{2}  &     6        \\ 
\bottomrule
\end{tabular}
\end{table}

In the first test, excluding two problems where no method found a solution (possibly due to it having become infeasible), \texttt{LAQP} was faster in $28$ problems ($64$\%) while \texttt{IPM} was faster in $16$ problems ($36$\%). On the second test, \texttt{LAQP} was faster in $29$ problems ($63$\%) while \texttt{IPM} was faster in $17$ problems ($37$\%).

\section{Conclusions}


%Neste artigo quebramos o paradigma de resolver problemas de
%Programao Linear usando mtodos de restries ativas do tipo
%Simplex ou mtodos do tipo Pontos interiores. Propusemos a ideia de
%usar uma abordagem do tipo Lagrangiano Aumentado para resolver este
%problema e discutimos diversas vantagens desta alternativa em
%relao a mtodos do tipo Pontos Interiores.
%
%Com nossa proposta abrimos um leque de possibilidades de
%investigao sobre o tema. O estudo da convergncia finita do
%algoritmo exterior, e a convergncia finita sem busca linear no
%algoritmo interno so questes importantes que devem ser estudadas
%em pesquisas futuras.
%
%Boas estimativas do ponto inicial e dos multiplicadores de Lagrange podem fazer o mtodo muito eficiente. 
%Acreditamos que isso possa ser usado em mtodos de programao linear ou quadrtica sequencial
%para programao no linear. O ponto inicial pooderia ser muito melhor aproveitado do que em mtodos
%de pontos interiores ou no mtodo simplex.

In this paper we considered an implementation of an augmented Lagrangian method for convex quadratic programming, where bound constraints are penalized and linear equality constraints are carried out to the subproblems. For solving the piecewise quadratic subproblems we employ a Newtonian method with exact line searches, where we were able to show finite convergence of both the inner and the outer algorithms. In our numerical experiments we compared our approach with a pure interior point approach, where we showed that although the interior point method was overall faster, our method was superior in solving a problem from near-optimal but non-interior initial points. This situation is relevant in at least three situations: sensitivity analysis, branch and bound methods and in sequential quadratic (or linear) programming methods for solving general nonlinear problems. This is due to the fact that our method does not need strict interiority or proximity to the central path in order to behave properly. It is also the case that the augmented Lagrangian approach was faster in circa $30\%$ of the problems, which is somewhat interesting.

The natural question that arises is whether our approach could be improved to be competitive with professional interior point codes. It is well known that sophisticated techniques for solving the linear systems arising in interior point iterations are available \cite{gondzio25}, either for the augmented system or for the normal equations, exploring sparsity and parallelization. All these techniques can also be employed for enhancing our augmented Lagrangian method, and we leave to a future study a more robust comparison in this sense. Also, it remains to be investigated how to fine tune the regularization parameter to allow our method to solve linear programming problems more consistently.

In the context of augmented Lagrangian methods for general nonlinear programming, our results are somewhat surprising in the sense that performing a second-order Lagrange multiplier update, at least near the solution, was much better than the usual first-order update, which is used in most well known implementations. Besides results about superlinear convergence under the second-order update \cite{yaxyuan,convbertsekas}, this update has not been much explored in the literature. Of course, its main drawback is the necessity of inverting the Hessian of the augmented Lagrangian function, but, at least when the inverse is readily available, incorporating the second-order update should outperform methods using only the first-order update. It is also interesting to investigate alternative updates that do not need computation of the inverse but that use information about the Hessian, say, following a quasi-Newton approach.


\section*{Acknowledgments}

We dedicate this paper in honor to the 70th birthday of Professor J. M. Martnez, who greatly influenced our careers. Moreover, we would like to thank him for important discussions we had in designing the original ideas of this work. In addition, we would like to thank Roger Behling, Ernesto Birgin and Thadeu Senne for their discussions along different stages of the trajectory of this paper.

\printbibliography
% \bibliographystyle{plain}
% \bibliography{biblio}

\end{document}
