\documentclass{article}
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{showlabels}
\usepackage{latexsym}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{siunitx}

\usepackage{color}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage[hyperref=true,
                       isbn=false,
                       style=numeric-comp,
                       giveninits=true,
                       maxbibnames=99]{biblatex}
\addbibresource{biblio.bib}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{cleveref}
\usepackage{booktabs}
\setlength{\textwidth}     {15.0cm} \setlength{\textheight}
{21.0cm} \setlength{\evensidemargin}{ 0.5cm}
\setlength{\oddsidemargin} { 0.5cm} \setlength{\topmargin}
{-0.5cm} \setlength{\baselineskip}  { 0.7cm}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\lambdau}{\underline{\lambda}}
\newcommand{\muu}{\underline{\mu}}
\newcommand{\subsetinf}{\displaystyle\mathop{\subset}_{\infty}}

\newcommand{\halmos}{\hfill $\;\;\;\Box$\\}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{hip}{H}[section]
\newtheorem{defi}{Definition}[section]

\begin{document}

\title{Towards an efficient penalty method for convex quadratic programming \footnote{This work was supported by FAPESP 2013/05475-7, 2017/18308-2 and CNPq.}}
\author{
  L. F. Bueno \thanks{Institute of Science and Technology, Federal
    University of S\~ao Paulo, S\~ao Jos\'e dos Campos-SP,
    Brazil. E-mail: lfelipebueno@gmail.com}
  \and
  G. Haeser \thanks{Department of Applied Mathematics, University of S\~ao Paulo, S\~ao Paulo-SP, Brazil. E-mail: ghaeser@ime.usp.br}
    \and
    L.-R. Santos \thanks{Department of Mathematics, Federal University of Santa Catarina, Blumenau-SC, Brazil. E-mail: l.r.santos@ufsc.br}
}

\date{\today}

\maketitle

\begin{abstract}

Interior point methods have attracted most of the attention in the recent decades for solving large scale convex quadratic programming problems. In this paper we take a different route as we present a penalty method for convex quadratic programming based on recent augmented Lagrangian developments for nonlinear programming. The motivation of this approach is that Newton Method can be efficient for minimizing a piecewise quadratic function. Moreover, since penalty methods do not rely on proximity to the central path, some of the inherent difficulties in interior point methods can be avoided. Also, a good starting point can be easily exploited, which can be relevant for solving subproblems arising from sequential quadratic programming. Some theoretical results on linear programming and strictly convex programming are presented. Numerical experiments on strictly convex quadratic problems formulated from the \textsc{Netlib} collection show that our method can be competitive with interior point methods, which suggests that more research on penalty methods should be conducted.\\

\noindent {\bf Key words:} Linear programming, convex quadratic programming, augmented Lagrangian, penalty methods, interior point methods\\

\noindent {\bf AMS Subject Classification:} 90C30, 49K99, 65K05.

\end{abstract}

\section{Introduction} \label{intro}

Since the 1940s, given its wide range of applications, linear programming problems have been very well studied. 
In particular, the first applications were made in the military sector, in the context of WWII, which boosted the study of numerical solutions of these problems.



In the present date, the state of the art linear programming solvers are interior point methods. These methods can be derived from the idea of minimizing a log-barrier subproblem with Newton method and they can be generalized for solving convex quadratic optimization problems. It is well known that an efficient interior point method should approximately follow the central path, otherwise, the method may fail due to the poor quadratic approximation of the log-barrier function near a non-optimal vertex.

In nonlinear programming, a dual approach to barrier methods are the classical penalty methods. Here, instead of the log-barrier function, one penalizes infeasibility with the $\ell_2$-norm squared. In this case, when dealing with quadratic programming, the subproblems are piecewise quadratic, and Newton method should perform well without the necessity of remaining close to the central path. The main contribution of this paper is investigating this possibility.

Given the enormous success of interior point methods since the work of Karmarkar in 1984, research in penalty methods have been overlooked in favor of barrier-type methods. This is not without merit, however, the goal of this paper is to show that very simple penalty methods can perform similarly to barrier-type methods.

Usually, in interior point methods, a sufficiently interior initial point must be computed in order for the method to perform well. The Mehrotra initial point strategy~\cite{Mehrotra:1992wr} is often the choice, even to the point that many interior point linear programming solvers do not even allow the user to specify a different initial point. This situation can be costly if one is solving a perturbation of a problem that has already been solved. A particular case is when a sequence of linear programming problems are solved, as in sequential linear programming methods for nonlinear optimization. In this case, the solution of a problem can give significative information about the solution of the next problem. Thus, not using the solution of the previous problem as an initial point for the next, is usually a bad decision. Recently some warm-start strategies for interior point methods have been presented, which are competitive with Simplex solvers~\cite{Yildirim:2002iy,John:2007jm}. However, even when an initial solution is given in a warm-started interior point method, some modification of the solution must occur in order to provide to the solver an initial point in a neighborhood of the central path. This drawback is not present in penalty methods, as any initial solution can be exploited in its entirety.

There has been many recent developments in penalty methods for nonlinear optimization, in particular, in augmented Lagrangian methods. In this paper, we revisit the convex quadratic programming problem in light of these recent augmented Lagrangian methods in order to propose an efficient method. Our algorithm will follow very closely an interior-point-like framework, in the sense that its core computational work will resort to the computation of interior-point-like Newton directions.

In Section 2, we recall some general augmented Lagrangian results, where we obtain some specialized results in the linear and convex quadratic cases. In Section 3, we present our strategy for solving the augmented Lagrangian subproblem. In Section 4, we present our numerical results.\\



\noindent {\bf Notation:} The symbol $\|\cdot\|$ denotes the Euclidean norm in $\R^n$. By $\R^n_+$ we denote the set of vectors in $\R^n$ with all components non-negative. The set of non-negative integers is denoted by $\N$. If $K \subseteq \N$ is an infinite sequence of indexes and $\lim_{k \in K} x^k = x$, we say that $x$ is a limit point of the sequence $\{x^k\}$.



\section{The Augmented Lagrangian method}  \label{ALm}

Let us consider the general nonlinear programming problem in the following form:
\begin{equation} \label{PNL}
\text{Minimize } f(x), \text{ subject to } h(x)=0, g(x)\leq0, H(x)=0, G(x)\leq0,
\end{equation}
where $f:\R^n\to\R, h:\R^n\to\R^m, g:\R^n\to\R^p, H:\R^n\to\R^{\bar{m}}, G:\R^n\to\R^{\bar{p}}$ are smooth functions.

The constraints $h(x)=0$ and $g(x)\leq 0$ are called the lower-level constraints, while the constraints $H(x)=0$ and $G(x)\leq 0$ are called the upper-level constraints. Given Lagrange multipliers approximations $\lambda\in\R^{\bar m}$, $\mu\in\R^{\bar p}_+$ and a penalty parameter $\rho>0$, the Powell-Hestenes-Rockafellar augmented Lagrangian function associated with the upper-level constraints is defined as
\begin{equation}
\label{auglag}
x\mapsto L(x,\rho,\lambda,\mu)=f(x)+\frac{\rho}{2}\left(\left\|H(x)+\frac{\lambda}{\rho}\right\|^2+\left\|\max\left\{0,G(x)+\frac{\mu}{\rho}\right\}\right\|^2\right).
\end{equation}

The augmented Lagrangian method as described in \cite{bmbook}, in each iteration, approximately solves the subproblem of minimizing the augmented Lagrangian function subject to the lower-level constraints.  Therefore, the set $F \equiv \{x\in\R^m\mid h(x)=0, g(x)\leq0\}$ is the feasible set of the subproblems. The Lagrange multipliers approximations are updated at each iteration  in a standard way and  the penalty parameter increases when progress, measured in terms of feasibility and complementarity, is not sufficiently made. More precisely, given the current iterate $x^k\in\R^m$, the current penalty parameter $\rho_k>0$, and the current Lagrange multipliers approximations $\lambda^k\in\R^{\bar{m}}$ and $\mu^k\in\R^{\bar{p}}_+$, a new iteration is computed in the following way:

\begin{itemize}
\item {\bf Step 1 (solve subproblem)}: from $x^k$, find an approximate solution $x^{k+1}$ of the problem:
\begin{equation} \label{subproblemaLA}
\text{Minimize } L(x,\rho_k,\lambda^k,\mu^k), \text{ subject to } x\in F.
\end{equation}
\item {\bf Step 2 (update multipliers)}: compute $\lambda^{k+1}$ in $[\lambda_{\min},\lambda_{\max}]$ and $\mu^{k+1}$ in $[0,\mu_{\max}]$.
\item {\bf Step 3 (update penalty)}: Set $V^{k+1}=\max\{G(x^{k+1}),-\mu^k/\rho_k\}$. If $$\|(H(x^{k+1}),V^{k+1})\|_\infty> \frac{1}{2}\|(H(x^{k}),V^{k})\|_\infty,$$ set $\rho_{k+1}=10\rho_k$, otherwise set $\rho_{k+1}=\rho_k$.
\end{itemize}

One of the most usual rules for updating the multipliers is the first order update in which $\lambda^{k+1}$ is computed as the projection of $\lambda^k+\rho_k H(x^{k+1})$ onto a safeguarded box $[\lambda_{\min},\lambda_{\max}]$ and $\mu^{k+1}$ as the projection of $\mu^k+\rho_k G(x^{k+1})$ onto a safeguarded box $[0,\mu_{\max}]$. Standard first- and second-order global convergence results are proved under weak constraint qualifications, depending whether subproblems are solved approximately up to first- or second-order, respectively. See, for instance, \cite{bhr} for details. If approximate global minimizers are found for the subproblems, one gets convergence to a global minimizer, and that is the main reason for using a safeguarded Lagrange multiplier (see \cite{bmbook}).

In terms of the global convergence theory, the choice of lower- and upper-level constraints can be done arbitrarily, however, the practical behavior of the method depends strongly on the quality of the optimization solver to minimize the augmented Lagrangian function subject to the lower-level constraints. The ALGENCAN implementation\footnote{Freely available at: \url{www.ime.usp.br/~egbirgin/tango}.} considers $F=\{x\in\R^n \mid l\leq x\leq u\}$ and penalizes all remaining constraints, using an active-set strategy with the spectral projected gradient choice to leave a non-optimal face, when solving the box-constrained subproblems.

In \cite{seco}, a radical shift of approach was suggested, by penalizing the box constraints and keeping equality constraints as subproblems' constraints. That is,  when solving a problem with constraints $h(x)=0,\,l\leq x\leq u$, the authors of \cite{seco} chose $l\leq x\leq u$ as the upper level constraints to be penalized and $h(x)=0$ as the lower level constraints. They then explored the simplicity of the KKT system when only equality constraints are present to develop a simple Newton method for solving the subproblems. Surprisingly, this simple approach had a performance compatible with ALGENCAN, a fine-tuned algorithm, on the CUTEst collection.

This approach is very similar to the interior point approach, where simple bound constraints are penalized with the log-barrier function, and Newton method is used for solving the subproblem with equality constraints. This similarities motivated us to compare the interior point and the augmented Lagrangian approaches in the simplest context, that is, when the constraints are linear and the objective function is convex and quadratic.

We will present an augmented Lagrangian algorithm for convex quadratic programming on the lines of the developments in \cite{seco}. That is, we are interested in the following problem:
\begin{equation} \label{PL}
\text{Minimize } \frac{1}{2}x^TQx+c^Tx \text{ subject to } Ax=b, \;\;\; l \leq x \leq
u,
\end{equation}
where $c,l,u \in \R^n$ with $l < u$, $b \in \R^m$, $A \in \R^{m \times n}$, and a positive semidefinite symmetric $Q\in\R^{n\times n}$ are given. We assume that $m<n$ and $A$ is full rank. %\textcolor{red}{Nao seria isso? matrix, $d^TQd \geq 0$ for all non null $d$ in the kernel of $A$.} 
The set $F$ is defined by the points that fulfill the equality constraints, that is,
$$F=\{x\in\R^n\mid Ax=b\}.$$

%Usually, two approaches are used for analyzing the global convergence of Augmented Lagrangian algorithms: one considering convergence to global minimizers and the other to stationary points. This is done by considering that approximate global minimizers of the subproblems are found or, respectively, approximate stationary points. Since we are only dealing with convex problems (and subproblems), where all stationary points are global minimizers, this distinction is irrelevant. Since it is relatively easy to check the KKT conditions at a given point, we have chosen to state that a point is an $\varepsilon$-approximate solution of the subproblem when it satisfies its KKT conditions up to a precision $\varepsilon$.
 

The algorithm presented in this section is a particular
case of the augmented Lagrangian method described previously, when applied to the convex quadratic programming problem \eqref{PL}, penalizing the box constraints and considering the equality constraints as the lower level constraints. An approximate solution for the subproblem will be one that satisfies the first-order stationarity conditions up to a given tolerance, that is, the norm of the residual of the corresponding nonlinear system of equations is at most the given tolerance. Due to the convexity of the problem, this will be enough to obtain convergence to a global minimizer.

% Inspired by the interior point approach, where only one newtonian iteration is employed at each subproblem, we include a possibility of aborting the solution of the subproblem and correcting the Lagrange multipliers approximations in order to avoid oversolving the subproblems.

We denote by $\lambda\in\R^m$ a Lagrange multiplier approximation associated with constraints $x\in F$, and $\mu_l\in\R^n_+$, $\mu_u\in\R^n_+$ Lagrange multipliers approximations associated with constraints $-x+l\leq 0$ and $x-u\leq 0$, respectively.

\begin{algorithm}[H]
  \caption{Augmented Lagrangian algorithm}
  \label{ALalg}
  \begin{algorithmic}
\State \noindent  \textbf{Step 0 (initialization)}: \\ Let $\rho_0=1$ and $\varepsilon \geq 0$ and set $k\leftarrow0$. Compute $(x^0,\lambda^0,\mu_l^0,\mu_u^0)\in F\times\R^m\times\R^n_+\times\R^n_+$.\\

\State \noindent \textbf{Step 1 (stopping criterion)}:\\ Set the primal residual $r_P=\max\{0,l-x^k,x^k-u\}$, the dual residual $r_D=Qx^k+c+A^\T\lambda^k-\mu_l^k+\mu_u^k$ and the complementarity measure $r_C=(\min(\mu_l^k,x^k-l),\min(\mu_u^k,u-x^k))$. Stop if $\|(r_P,r_D,r_C)\|_\infty\leq\varepsilon$.\\

%If $x^0 \in F$, $\mu_l^0$, and
%$\mu_u^0$ are given, go to Step 2. Else, define $x^0$ as the vector
%$x$, where $(x,\lambda)$ solves
%\begin{equation} \label{Ptoinicialx0}
%\left(
%\begin{array}{cc}
%\epsilon_{reg} I& A^T\\
%A & 0
%\end{array}
%\right)\left(
%\begin{array}{c}
%x \\
%\lambda
%\end{array}
%\right) = \left(
%\begin{array}{c}
%-c\\
%b
%\end{array}
%\right).
%\end{equation}
%Define $\mu_l^0=P_{[0,\mu_{\max}]}\left(\epsilon_{reg}(l-x^0)\right)$ and
%$\mu_u^0=P_{[0,\mu_{\max}]}\left(\epsilon_{reg}(x^0-u)\right)$.
        
    \State  \noindent \textbf{Step 2 (solve subproblem)}:\\
    Find an approximate solution $x^{k+1}$ with corresponding approximate Lagrange multiplier $\lambda^{k+1}$ with a tolerance $\varepsilon_{k+1}$ of the subproblem
\begin{equation}\begin{array}{ll} \label{subprobLA}
\text{Minimize}&
L(x,\rho_k,\mu_l^k,\mu_u^k)=\frac{1}{2}x^TQx+c^Tx+\frac{\rho_k}{2}\left(\left\|\max\left\{-x+(l+\frac{\mu_l^k}{\rho_k}),0\right\}\right\|^2+
\left\|\max\left\{x-(u-\frac{\mu_u^k}{\rho_k}),0\right\}\right\|^2\right),\\
\text{subject to}& Ax=b.
\end{array}
\end{equation}

    \State  \noindent \textbf{Step 3 (update multipliers)}:\\    
    Compute $\mu_l^{k+1},\mu_u^{k+1}\in[0,\mu_{\max}]$, for some fixed $\mu_{\max}>0$. This procedure can be a first- or a second-order one and will be specified afterwards.\\


    \State  \noindent \textbf{Step 4 (update penalty)}: If
    $$\|\max\{-\mu_l^k/\rho_k,-\mu_u^k/\rho_k,x^{k+1}-u,l-x^{k+1}\}\|_{\infty}>
    \frac{1}{2}\|\max\{-\mu_l^{k-1}/\rho_{k-1},-\mu_u^{k-1}/\rho_{k-1},x^{k}-u,l-x^{k}\}\|_{\infty}$$ set $\rho_{k+1}=10 \rho_k,$ otherwise set $\rho_{k+1}=\rho_k$.\\

    \State  \noindent \textbf{Step 5. (repeat)}: Update $k \leftarrow k+1$ and go to Step~1.\\

  \end{algorithmic}
\end{algorithm}

We will consider two options for the Lagrange multiplier update in Step 3. The first-order update will be defined as $\mu_l^{k+1}=\max\{0,\mu_l^k+\rho_k(-x^k+l)\}$ and $\mu_u^{k+1}=\max\{0,\mu_u^k+\rho_k(u+x^k)\}$. With this update, it has been proved in \cite{LAgnep} that the sequence $\{(\mu_l^k,\mu_u^k)\}$ is bounded. More specifically, the first-order Lagrange multiplier update is bounded for problems that satisfy the quasinormality constraint qualification (which includes linear constraints). This indicates that this update is acceptable for sufficiently large $\mu_{\max}$, hence this parameter can be dropped.



Our second-order Lagrange multiplier update is used when dealing with separable strictly convex quadratic problems. The update formula is obtained according to the ideas presented in \cite{yaxyuan,bertsekas,buys}. For this purpose we need to introduce a notation to refer to the coordinates associated with the active box constraints at a point $x^{k+1}$, as well as with parts of vectors and matrices corresponding to these coordinates. Let $I_l=\{i: x^{k+1}_i\leq l_i\}$ and $I_u=\{i: x^{k+1}_i\geq u\}$ be the indexes of active or infeasible box constraints. Let us define $\mathcal{I}_{I_l}$  as the $|I_l|\times n$ matrix formed by the $i$-th line of the $n\times n$ identity, $i\in I_l$. Similarly we define $\mathcal{I}_{I_u}$. 

Let $Z=\left(\begin{array}{c} -\mathcal{I}_{I_l}\\\mathcal{I}_{I_u}\\A\end{array}\right)$ and $M=Z(Q+H)^{-1}Z^\mathtt{T}$, where $H$ is the Hessian of $$\frac{\rho_k}{2}\left(\left\|\max\left\{-x+\left(l+\frac{\mu_l^k}{\rho_k}\right),0\right\}\right\|^2+
\left\|\max\left\{x-\left(u-\frac{\mu_u^k}{\rho_k}\right),0\right\}\right\|^2\right)$$ at $x^{k+1}$, taken in the semismooth sense, namely, $H$ is a diagonal matrix with its $i$-th entry equal to $0$ if $l_i<x_i^{k+1}<u_i$ and equal to $\rho_k$ otherwise. \textcolor{red}{H é a Hessiana do pedaço que coloquei ou é sem o deslocamento mesmo? Se for com o deslocamento, a explicação de onde é zero tem que ser consertada}

When $Q$ is diagonal and positive definite (separable and stricly convex problem), the expression for $M$ can be easily computed explicitly, and this is the case where we use the second-order update.

To update the Lagrange multiplier approximations $\mu^k_l$ and $\mu^k_u$, we solve the linear system \begin{equation}\label{sist2orderupdate}
M\left(\begin{array}{c}d^1\\d^2\\d^3\end{array}\right)=\left(\begin{array}{c}l_{I_l}-x^{k+1}_{I_l}\\x^{k+1}_{I_u}-u_{I_u}\\b-Ax^{k+1}\end{array}\right),
\end{equation}
where $d^1\in\R^{|I_l|}, d^2\in\R^{|I_u|}$, and $d^3\in\R^m$. The notation $v_{I_l}\in\R^{|I_l|}$ corresponds to the vector with components $v_i, i\in I_l$ of the vector $v\in\R^n$. Similarly for $I_u$. We then define $[\mu_l^{k+1}]_{I_l}=\max\{0,[\mu_l^{k}]_{I_l}+d^1\}$ and $[\mu_u^{k+1}]_{I_u}=\max\{0,[\mu_u^{k}]_{I_u}+d^2\}$, keeping $[\mu_l^{k+1}]_i=[\mu_l^k]_i$ and $[\mu_u^{k+1}]_i=[\mu_u^k]_i$ for the remaining indexes. This is the standard second-order Lagrange multiplier update when considering that all constraints are penalized. However, by following the approach of \cite{yaxyuan} but considering constrained subproblems, we arrived at the same formula.

If $Q$ is not positive definite or the inverse of $Q+H$ is not easily computable, we advocate the use of the first-order update only. In our implementation we used the first-order update if the solution of \eqref{sist2orderupdate} is not accurate.


%\begin{color}{red}We say that a point is an approximated solution of an optimization problem if we can ensure that the 
%KKT conditions holds with precision $\epsilon$. We stop Algorithm \ref{ALalg} with a fixed non negative precision.
%Moreover, we require that the precison $\epsilon_k \in \R_+$, used in the subproblems, tends to zero when $k$ goes to infinite.
%Since problem \eqref{PL} is convex,  when the algorithm stops at an iteration $k_0$,
%we have an approximated solution of \eqref{PL}.\end{color}
In order to analyze the asymptotic behavior of the algorithm, 
we consider that it only stops at an iteration $k_0$ when an exact solution is found, that is, we consider $\varepsilon=0$ in the stopping criterion. Even in this case, we consider that an infinite sequence is generated with $x^{k}=x^{k_0}$ for all $k > k_0$.

In augmented Lagrangian approaches where the box constraints $ l \leq x \leq
u$ are not penalized, it is straightforward that the subproblems are well defined and
that the generated sequence remains in a compact set. The following
lemmas ensure that these properties hold for Algorithm \ref{ALalg}, even though box constraints are not necessarily preserved. This is a consequence of dealing with quadratic convex problems. We start by showing well-definiteness:

%\begin{lemma} \label{Lvaiinfinitosexvai}
%The objective function of the subproblem
%$L(x,\rho_k,\mu_l^k,\mu_u^k)$, as a function of $x$, tends to
%infinity when the norm of $x$ goes to infinity.
%\end{lemma}
%
%\noindent {\it Proof} Note that   $L(x,\rho_k,\mu_l^k,\mu_u^k)$ is a composition of a bounded below quadratic term  plus 
%separable terms in relation to each of the variables. That is,
%$$L(x,\rho_k,\mu_l^k,\mu_u^k)= \frac{1}{2}x^TQx+\sum_{i=1}^n L_i^k(x_i),$$
%where
%\begin{equation}\label{defLik}
%L_i^k(x_i)=c_ix_i+\frac{\rho_k}{2}\left(\max\left\{l_i-x_i+\frac{[\mu_l^k]_i}{\rho_k},0\right\}^2+
%\max\left\{x_i-u_i+\frac{[\mu_u^k]_i}{\rho_k},0\right\}^2\right).
%\end{equation}
%
%
%When $\|x\|$ tends to infinity we have that at least one
% coordinate  of $x$ will have its absolute value arbitrarily large.
%Thus, for these components, we have that $x_i<l_i$(and
%hence smaller than $l_i+\frac{[\mu_l^k]_i}{\rho_k}$) or greater than
%$u_i$.
%
%The functions $L_i^k(x_i)$ are strictly convex quadratics for
%$|x_i|$ large enough. So we  have that $L_i^k(x_i)$ tends to
%infinity when $|x_i|$ goes to infinity. If $|x_i|$ is bounded,  by continuity,
%we have that $L_i^k(x_i)$ is also bounded, and so we have the sum
%$\sum_{i=1^n} L_i^k(x_i)=L(x,\rho_k,\mu_l,\mu_u)$ is unbounded
%superiorly when $\|x\|$ tends to infinity. Since $Q$ is semidefinite, the term $\frac{1}{2}x^TQx$ is bounded below by zero and so we have the desired result. \halmos

\begin{lemma} \label{teobemdef} If $F \neq \emptyset$ then subproblem \eqref{subprobLA} is always well defined.
\end{lemma}

\noindent {\it Proof.} It is easy to see that the objective function of the subproblem, $x\mapsto L(x,\rho_k,\mu_l^k,\mu_u^k)$, tends to
infinity when the norm of $x$ goes to infinity, that is, it is coercive, and the result follows from a well known existence result \cite{bertsekas}.\halmos

%Since the constraints $Ax=b$ are consistent,
%there exist feasible points for the subproblem. We denote by
%$\bar{x}$ one of these solutions.
%
%By Lemma \eqref{Lvaiinfinitosexvai} $\lim_{\|x\| \to \infty}
%L(x,\rho_k,\mu_l,\mu_u)= \infty$ and so there is an closed ball
%$B_k$ such that
%$L(x,\rho_k,\mu_l,\mu_u)>L(\bar{x},\rho_k,\mu_l,\mu_u)+1$ for every
%$x \notin B_k$. Obviously, $\bar{x} \in  B_k$ and therefore the set
%$$R_k=F \cap B_k$$
%is compact and not empty. By the continuity of
%$L(x,\rho_k,\mu_l^k,\mu_u^k)$ there exists a minimizer of
%$L(x,\rho_k,\mu_l^k,\mu_u^k)$ at $R_k$,  and, by the definition of
%$B_k$, this point will also be a minimizer of the subproblem
%\eqref{subprobLA}. \halmos


%In the previous theorem the set $B_k$ depends on the penalty
%parameter $\rho_k$, and on the Lagrange multipliers estimates
%$\mu_l^k$ and $\mu_u^k$. 
The next result shows that it is actually
possible to ensure that a sequence generated by Algorithm \ref{ALalg} lies in a compact set:

\begin{lemma} \label{ficacompato} If $F \neq \emptyset$ then a sequence
$\{x^k\}$ generated by Algorithm \ref{ALalg} lies in a compact
set.
\end{lemma}

\noindent {\it Proof.} By Lemma \ref{teobemdef}, the sequence $\{x^k\}$ is well defined. Let us show that
there is a single compact set containing all iterates $x^k$.




Let
$$L(x,\rho_k,\mu_l^k,\mu_u^k)= \frac{1}{2}x^TQx+\sum_{i=1}^n L_i^k(x_i),$$
where
\begin{equation}\label{defLik}
L_i^k(x_i)=c_ix_i+\frac{\rho_k}{2}\left(\max\left\{l_i-x_i+\frac{[\mu_l^k]_i}{\rho_k},0\right\}^2+
\max\left\{x_i-u_i+\frac{[\mu_u^k]_i}{\rho_k},0\right\}^2\right).
\end{equation}


For each function $L_i^k(x_i)$ we have that
\begin{equation}\label{defLi}
\begin{array}{ccl}
L_i^k(x_i)&\geq& c_i
x_i+\frac{\rho_0}{2}\left(\max\left\{l_i-x_i+\frac{[\mu_l^k]_i}{\rho_k},0\right\}^2+
\max\left\{x_i-u_i+\frac{[\mu_u^k]_i}{\rho_k},0\right\}^2\right)\\[2pt]
&\geq& c_i x_i+\frac{\rho_0}{2}\left(\max\left\{l_i-x_i,0\right\}^2+
\max\left\{x_i-u_i,0\right\}^2\right) \equiv L_i(x_i).
\end{array}
\end{equation}


Note that $L_i(x_i)$ does not depend on $k$ and, since
\begin{equation}\label{limLi}
L_i(x_i) \geq \left\{
\begin{array}{l}
c_i l_i - \frac{c_i^2}{2\rho_0}, \; \text{ if } \; c_i\geq 0,\\[5pt]
c_i u_i - \frac{c_i^2}{2\rho_0}, \; \text{ if } \; c_i\leq 0,
\end{array}
\right.
\end{equation}
it is bounded below.

On the other hand, the function $L_i^k(x_i)$ is bounded above by
$$L_i^k(x_i)\leq c_ix_i+\frac{\rho_k}{2}h_i(x_i),$$
where
$$h_i(x_i) \equiv \max\left\{l_i-x_i+\frac{\mu_{\max}}{\rho_0},0\right\}^2+
\max\left\{x_i-u_i+\frac{\mu_{\max}}{\rho_0},0\right\}^2.$$

Let $\bar{x} \in \R^n$ be a feasible
point for the subproblems, that is $A\bar{x}=b$. Denoting $\bar{h}=
\sum_{i=1}^n h_i(\bar{x}_i)$ we have that
\begin{equation}\label{eq3xkcompacto}
L(\bar{x},\rho_k,\mu_l^k,\mu_u^k) \leq \frac{1}{2} \bar{x}^T Q \bar{x}+ c^T\bar{x}+\frac{\rho_k}{2} \bar{h}.
\end{equation}


Now, suppose by contradiction that the sequence $\{x^k\}$ is unbounded.
In this case there would be an index $i$ and an infinite set $K
\subset \N$ such that
$$\lim_{k \in K} x^k_i=- \infty \;\; \text{ or } \lim_{k \in K} x^k_i=
\infty.$$
Without loss of generality we assume that $\lim_{k \in K}
x^k_1= \infty$.

By \eqref{limLi}  there exists $\bar{L}$ such that $\bar{L} < \sum_{i=2}^n L_i^k(x_i)$.
Therefore,
\begin{equation}\label{eq1xkcompacto}
\bar{L}+c_1x_1+ \frac{\rho_k}{2} \max\left\{x_1-u_1,0\right\}^2 <
\frac{1}{2} x^T Q x+\sum_{i=1}^n L_i^k(x_i)=L(x,\rho_k,\mu_l^k,\mu_u^k).
\end{equation}

Combining \eqref{eq3xkcompacto} and  \eqref{eq1xkcompacto} we will show that $L(x^k,\rho_k,\mu_l^k,\mu_u^k)$ is significantly greater than  $L(\bar{x},\rho_k,\mu_l^k,\mu_u^k)$. In order to do this we will show that,   given any $p>0$, for $x_1$ large enough
\begin{equation}\label{eq15xkcompacto}
 \frac{1}{2} \bar{x}^T Q \bar{x}+ c^T\bar{x}+\frac{\rho_k}{2} \bar{h}+p \leq \bar{L}+c_1x_1+ \frac{\rho_k}{2} \max\left\{x_1-u_1,0\right\}^2.
\end{equation}
Since $\rho_k \geq \rho_0$, if $x_1^k > u_1+ \sqrt{\bar{h}}$, \eqref{eq15xkcompacto}  holds if  
\begin{equation}\label{eq2xkcompacto}
\frac{\rho_0}{2} [(x_1-u_1)^2- \bar{h}]+c_1x_1+\bar{C} \geq 0,
\end{equation}
where $\bar{C}=\bar{L}-\frac{1}{2} \bar{x}^T Q \bar{x}- c^T\bar{x} -p$.



Defining   $\Delta=c_1^2-2\rho_0 \bar{C}+ \rho_0^2 \bar{h}-2\rho_0c_1u_1$ we have that, whenever $\Delta < 0$, \eqref{eq2xkcompacto} is satisfied for all $x_1$. If $\Delta >0$, \eqref{eq2xkcompacto} holds for $x_1 \geq  u_1+\frac{-c_1+\sqrt{\Delta}}{\rho_0}$. Therefore, $x_1^k > \max\{u_1+\frac{-c_1+\sqrt{\Delta}}{\rho_0}, u_1+ \sqrt{\bar{h}}\}$ ensures that $L(x^k,\rho_k,\mu_l^k,\mu_u^k) > L(\bar{x},\rho_k,\mu_l^k,\mu_u^k) + p.$
Since $p$ could be arbitrarily large, this contradicts  the fact that $x^k$ is an approximate minimizer of the subproblem. \halmos



We next recall the standard global convergence theory of Algorithm \ref{ALalg} as described in \cite{bmbook,abms}. The difference is that convexity implies that stationary points are global solutions. The main result is that every limit point of a sequence
generated by the algorithm is a solution of the original problem \eqref{PL}, provided the feasible set is non-empty.


\begin{theorem} \label{teoachasol} Assume that
the feasible set of problem \eqref{PL} is non-empty, then every
limit point of a sequence $\{x^k\}$ generated by Algorithm
\ref{ALalg} is a solution of problem \eqref{PL}.
\end{theorem}

\noindent {\it Proof.} First, note that the linearity of the constraints trivially implies that the constant rank constraint qualification (CRCQ, \cite{crcq}) holds. In particular, weaker constraint qualifications such as CPLD \cite{cpld,rcpld}, CPG \cite{cpg} or CCP \cite{ccp} also hold.


By \cite[Corollary 6.2]{abms} we have that  if $x^*$ is a
limit point of the sequence $\{x^k\}$ then it is a stationary point
of the problem
\begin{equation} \label{probviabil}
\text{Minimize } \left(\left\|\max\left\{l-x,0\right\}\right\|^2+
\left\|\max\left\{x-u,0\right\}\right\|^2\right) \text{ subject to }
Ax=b.
\end{equation}
Since this problem is convex, we can ensure that $x^*$ is a global
minimizer of \eqref{probviabil}. Thus, by the non-emptyness of the feasible set, $x^*$ is feasible for problem \eqref{PL}.

Hence, by \cite[Corollary 6.1]{abms}, $x^*$ is stationary for \eqref{PL}. By convexity, $x^*$ is a solution of problem \eqref{PL}. \halmos

Note that combining Lemma \ref{ficacompato} and Theorem \ref{teoachasol}, any sequence generated by the algorithm must have at least one limit point, which must be a solution of problem \eqref{PL}. Hence, one solution of the original problem is necessarily found by the algorithm. It is also a trivial consequence of Theorem \ref{teoachasol} that if problem \eqref{PL} has a unique solution, than a sequence generated by the algorithm is necessarily convergent to the solution.

%The next result shows that one of the solutions of the original
%problem \eqref{PL} is found by Algorithm \ref{ALalg}.
%
%\begin{cor} \label{achsolPL}
%  \emph{One of the solutions of the original problem
%  \eqref{PL} is a limit point of $\{x^k\}$.}
%\end{cor}
%
%\noindent {\it Proof.}  
%By Lemma \ref{ficacompato} the sequence  $\{x^k\}$ has at least one
%limit point, which, by Theorem \eqref{teoachasol}, is a solution of
%problem \eqref{PL}. \halmos
%
%
%\begin{cor} \label{convsolunica} 
%\emph{If problem \eqref{PL} has a
%unique solution $x^*$ the sequence $\{x^k\}$  converges to $x^*$.}
%\end{cor}
%
%\noindent {\it Proof.}  
%By Lemma \ref{ficacompato} the sequence  $\{x^k\}$ remains in a compact set.
%By Theorem \ref{teoachasol} all the limit ponits of $\{x^k\}$ are solutions of \eqref{PL}. 
%Once $x^*$ is the unique solution of \eqref{PL}, the sequence $\{x^k\}$  converges to $x^*$. \halmos

We end this section by showing that the algorithm finds a solution in finite time provided that subproblems are solved exactly ($\varepsilon_k=0$ for all $k$) and that the sequence $\{x^k\}$ converges to a regular and strictly complementary solution. This is an interesting property of the augmented Lagrangian method that is not enjoyed by pure interior point methods, that is, results of this type are available for interior point methods only by the addition of particular procedures employed at the end of the execution. See \cite{bixby,vavasis,Ye1992}. Our results are similar to the finite termination of a Newton method for piecewise linear systems described in \cite{yunier}.

%The next results shows that the whole sequence $\{x^k\}$ converges
%to the solution in finite time if \eqref{PL} is a Linear Programming problem ($Q=0$) or a strictly convex Quadratic problem ($Q > 0$). For this purpose we need some
%assumptions based on the concepts described as follows.

\begin{defi} \label{hipregular} Given $x \in \R^n$, let $I^l_x$ and $I^u_x$
be matrices whose columns are the canonical vectors $e_i \in \R^n$
such that $x_i=l_i$ and $x_i=u_i$, respectively. The point $x$ is
said to be regular for problem \eqref{PL} if the columns of the
matrix   $[A^T \,\,\, -I^l_x \,\,\, I^u_x]$ are linearly independent. Hence, a regular stationary point implies that there are unique $\lambda\in\R^m,\mu_l\in\R^n_+$ and $\mu_u\in\R^n_+$ that satisfy $$Qx+c+A^T \lambda- \mu_l+\mu_u=0,$$ with $[\mu_l]_i=0$ if $x_i>l_i$ and $[\mu_u]_i=0$ if $x_i<u_i$. A stationary point such that $[\mu_l]_i>0$ for all $i$ such that $x_i=l_i$, and $[\mu_u]_i> 0$ for all $i$ such that $x_i=u_i$ is said to satisfy strict complementarity.
\end{defi}

%\noindent{\bf Remark:} If  $x$ is a regular stationary point for the
%problem \eqref{PL} there are unique $\mu_l$ and $\mu_u$ in $\R^n_+$
%such that $[\mu_l]_i=0$ if $x_i>l_i$, $[\mu_u]_i=0$ if $x_i<u_i$,
%and
%$$Qx+c+A^T \lambda- \mu_l+\mu_u=0.$$
%
%\begin{defi} \label{hipcompestrita} We say that a stationary point $x \in \R^n$, for
%which there  exist $\mu_l$ and $\mu_u$ such that
%$$Qx+c+A^T \lambda -\mu_l+\mu_u=0,$$
%satisfies the strict complementarity if $[\mu_l]_i>0$ for all $i$
%such that $(x_i-l_i)=0$ and $[\mu_u]_i> 0$ for all $i$ such that
%$(x_i-u_i)=0$.
%\end{defi}



\begin{lemma} \label{LemaCompcertas} Assume that the sequence $\{x^k\}$ 
converges to $x^*$ and that $x^*$ is a regular solution of  problem \eqref{PL}  that satisfies strict complementarity with $\mu_l^* <
\mu_{\max}$ and $\mu_u^* < \mu_{\max}$. Then, there
exists $k_0 \in \N$ such that, for all $k \geq k_0$, $\mu^{k}_l>0 \Leftrightarrow
\mu^*_l>0$, $\mu^{k}_u > 0 \Leftrightarrow \mu^*_u>0$,
$x^{k}_i<l_i+\frac{[\mu^{k}_l]_i}{\rho_{k}}\Leftrightarrow
x^*_i=l_i$ and
$x^{k}_i>u_i-\frac{[\mu^{k}_u]_i}{\rho_{k}}\Leftrightarrow
x^*_i=u_i$. 
 
\end{lemma}

\noindent {\it Proof.}  Since $x^*$ is a regular solution,  $\mu_l^* < \mu_{\max}$ and
$\mu_u^* < \mu_{\max}$, we have, by Lemmas 7.1 and 7.2  of \cite{bmbook},
that $\lim \mu^k_l=\mu^*_l$ and $\lim \mu^k_u=\mu^*_u$. So, by the strictly complementarity, we have that
that $\mu^{k}_l>0 \Leftrightarrow
\mu^*_l>0$, $\mu^{k}_u > 0 \Leftrightarrow \mu^*_u>0$ for $k$ large enough.

By Theorem 7.2  of \cite{bmbook}, $\rho_k = \bar{\rho}$ for $k$ large enough. Thus, $\frac{[\mu^{k_0}_l]_i}{\rho_{k_0}}$
and $\frac{[\mu^{k_0}_u]_i}{\rho_{k_0}}$ are bounded away from zero if $x_i^*=l_i$ or  $x_i^*=u_i$. Since  $\{x^k\}$ 
converges to $x^*$, we have that $x^{k}_i<l_i+\frac{[\mu^{k}_l]_i}{\rho_{k}}\Leftrightarrow
x^*_i=l_i$ and
$x^{k}_i>u_i-\frac{[\mu^{k}_u]_i}{\rho_{k}}\Leftrightarrow
x^*_i=u_i$ for $k$ large enough. \halmos

\begin{lemma} \label{convfinitaLA} Assume that subproblems \eqref{subprobLA} are solved exactly ($\varepsilon_k=0$ for all $k$),
and that a sequence $\{x^k\}$ generated by Algorithm \ref{ALalg}
converges to $x^*$, which is a regular solution of  problem \eqref{PL}  that satisfies strict complementarity with $\mu_l^* <
\mu_{\max}$ and $\mu_u^* < \mu_{\max}$. Then, there exists $k_1$ such
  that $\mu^{k_1}_l=\mu_l^*$ and $\mu^{k_1}_u=\mu_u^*$.
\end{lemma}


\noindent {\it Proof.}  
By Lemma \ref{LemaCompcertas} we have that there exists $k_0 \in K$ such that, for all $k\geq k_0$, 
\begin{equation} \label{compcertasl}
\left[\max\left\{l-x^{k_0}+\frac{\mu^{k_0}_l}{\rho_{k_0}},0\right\}\right]_i= \left\{ 
\begin{array}{c}
\left[l-x^{k_0}+\frac{\mu^{k_0}_l}{\rho_{k_0}}\right]_i, \text{ if } x^*_i=l_i\\ 
0, \text{otherwise}
\end{array}
\right.
\end{equation}
and
\begin{equation} \label{compcertasu}
\left[\max\left\{x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}},0\right\}\right]_i= \left\{ 
\begin{array}{c}
\left[x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}}\right]_i, \text{ if } x^*_i=u_i\\ 
0, \text{otherwise}
\end{array}
\right. 
\end{equation}

Since problem \eqref{subprobLA} is linearly constrained, there exists $\lambda^{k_0} \in \R^m$ such
that $(x^{k_0},\lambda^{k_0})$ satisfies the system
\begin{equation} \label{KKTemK0}
c+A^T \lambda^{k_0}- \rho_{k_0}\max\left\{l-x^{k_0}+\frac{\mu^{k_0}_l}{\rho_{k_0}},0\right\}+
\rho_{k_0}\max\left\{x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}},0\right\}=0. 
\end{equation}


Since $x^*$ is a KKT regular point for \eqref{PL}, there exists $(\lambda^*,\bar{\mu}_l^*,\bar{\mu}_u^*)$ 
unique solution for the system 
\begin{equation}\label{multiplunicos}
[A^T \,\,\, -I^l_{x^*} \,\,\, I^u_{x^*}] \left(\begin{array}{c}
\lambda\\
\mu_l\\
\mu_u
\end{array}\right)
=-c,
\end{equation}
and $[\mu_l^*]_i=[\bar{\mu_l}^*]_i$ if $x^*_i=l_i$ and  $[\mu_l^*]_i=0$ otherwise and 
$[\mu_u^*]_i=[\bar{\mu_u}^*]_i$ if $x^*_i=u_i$ and  $[\mu_u^*]_i=0$ otherwise.

So, we have that $\lambda^{k_0}=\lambda^*$ and, by \eqref{compcertasl}, \eqref{compcertasu} and \eqref{KKTemK0},
$\rho_{k_0}\max\left\{l-x^{k_0}+\frac{\mu^{k_0}_l}{\rho_{k_0}},0\right\}=\mu_l^*$ and 
$\rho_{k_0}\max\left\{x^{k_0}-u+\frac{\mu^{k_0}_u}{\rho_{k_0}},0\right\}=\mu_u^*$.


By \eqref{compcertasl}, \eqref{compcertasu} and the definition of updating rule for the Lagrange multipliers we
have that
$$\mu_l^{k_0+1}=\mu_l^*$$
and
$$\mu_u^{k_0+1}=\mu_u^*.$$ \halmos


We are now ready to prove the finite convergence of the method:

\begin{theorem} \label{convfinitaLA2} Assume that the subproblems \eqref{subprobLA} are solved exactly,
the sequence $\{x^k\}$ 
converges to $x^*$, which
is a regular solution of  problem \eqref{PL}  and satisfies the strict complementarity  with $\mu_l^* < \mu_{\max}$ and $\mu_u^* < \mu_{\max}$. Then, there exists $k_1$ such
that $x^{k_1}=x^*$.
\end{theorem}


\noindent {\it Proof.}  
Since $x^{k_0+1}$ is a solution of the linearly constrained problem \eqref{subprobLA}, there exists $\lambda^{k_0+1} \in \R^m$ such
that $(x^{k_0+1},\lambda^{k_0+1})$ satisfies the system
\begin{equation} \label{KKTemK0mais1}
\begin{array}{c}
c+A^T \lambda^{k_0+1}- \rho_{k_0}\max\left\{l-x^{k_0+1}+\frac{\mu^*_l}{\rho_{k_0}},0\right\}+
\rho_{k_0}\max\left\{x^{k_0+1}-u+\frac{\mu^*_u}{\rho_{k_0}},0\right\}=0, \\[5pt]
A x^{k_0+1}=b.
\end{array}
\end{equation}

By the uniqueness of the solution of \eqref{multiplunicos} and Lemma \eqref{LemaCompcertas} we have that 
$x^{k_0+1}_i=l_i \Leftrightarrow x^*_i=l_i$ and $x^{k_0+1}_i=u_i \Leftrightarrow x^*_i=u_i$. So
$x^{k_0+1}$ is a KKT point for \eqref{PL} and thus it is a solution of \eqref{PL}.
\halmos

Note that if problem \eqref{PL} has a unique solution and this solution is regular and satisfies strict complementarity, with $\mu_{\max}$ sufficiently large, then the algorithm necessarily finds this solution in finite time.

%\begin{cor} \label{convfinitasolunica}
%  \emph{If  the subproblems \eqref{subprobLA} are solved exactly, the original problem
%  \eqref{PL} has a unique solution and this solution is regular and satisfies the strict complementarity  with $\mu_l^* <
%\mu_{\max}$ and $\mu_u^* < \mu_{\max}$, then Algorithm \ref{ALalg} finds
%  the solution  in finite time.}
%\end{cor}
%\noindent {\it Proof.}  By Corollary \eqref{convsolunica} we have that $\{x^k\}$ converges to the solution.
%Since the solution is regular and satisfies the strict complementarity  with $\mu_l^* <
%\mu_{\max}$ and $\mu_u^* < \mu_{\max}$, we have, by Theorem \ref{convfinitaLA2}, that the convergence is finite. \halmos


\section{Solving the subproblem}  \label{basicmethod}

In this section we discuss how to solve  subproblem
\eqref{subprobLA} of the Augmented Lagrangian method. Our main idea consists in using a Newton-type strategy. This approach is motivated by the fact that the constraints are linear and the objective function is piecewise quadratic. More precisely, we will solve the subproblem by Newton's method with exact line search.

When solving the subproblem, the indexes $k$ of $\rho_k$, $\mu_l^k$ and $\mu_u^k$
are fixed in \eqref{subprobLA}. Therefore, to simplify the notation,
we will suppress these indexes and redefine the constants
$l=l+\frac{\mu_l^k}{\rho_k}$ and   $u=u-\frac{\mu_u^k}{\rho_k}$.
Thus, throughout this section, we consider the objective function of
the subproblem as
$$L(x)=\frac{1}{2}x^TQx+c^Tx+\frac{\rho}{2}\left(\left\|\max\left\{l-x,0\right\}\right\|^2+
\left\|\max\left\{x-u,0\right\}\right\|^2\right),$$
which must be minimized subject to $x\in F$.

We will consider, from now on, that $Q$ is positive definite. If $Q$ is only positive semidefinite, our approach can still be carried out, but for the well-definiteness of the Newton direction one must choose a suitable regularization strategy.

\begin{algorithm}[H]
  \caption{ (for solving the subproblems)}
  \label{solvesubp}
  \begin{algorithmic}
\State \noindent  \textbf{Step 0 (initialization)}: \\ Let $\varepsilon\geq0$ and set $k\leftarrow0$. Set $(x^0,\lambda^0)\in F\times\R^m$ as the current outer iterate.\\

\State \noindent \textbf{Step 1 (stopping criterion)}:\\ Set $r=Qx^k+c+A^\T\lambda^k-\rho\max\{l-x,0\}+\rho\max\{x-u,0\}$ and stop if $\|r_k\|_\infty\leq\varepsilon$.\\

       
    \State  \noindent \textbf{Step 2 (compute Newton direction)}:\\
    Solve the linear system
    \begin{equation}\label{newton}
    \left(\begin{array}{cc}Q+H^k&A^\mathtt{T}\\A&0\end{array}\right)\left(\begin{array}{c}x_{trial}\\ \lambda^{k+1}\end{array}\right)=-c+v^k,
    \end{equation}
    where $v_i^k=\rho u_i$ if $x_i^k\geq u_i$, $v_i^k=\rho l_i$, if $x_i^k\leq l_i$, and $v_i^k=0$ otherwise. Also, $H^k$ is diagonal with $H^k_{ii}=0$ if $l_i<x_i^k<u_i$, and $H_{ii}^k=\rho$, otherwise.
    \State  \noindent \textbf{Step 3 (line search)}:\\
Set $d^k=\gamma(x_{trial}-x^k)$, where $\gamma=\min\left\{1,\frac{100}{\|x_{trial}-x^k\|}\right\}$. Find $t^*$ as the global minimizer of the one-dimensional piecewise quadratic $t\mapsto L(x^k+td^k)$ and compute $t_k$ as the projection of $t^*$ onto the safeguarded interval $[-\min\{\|d^k\|,10^{-6}\},1+\|d^k\|]$.\\

    \State  \noindent \textbf{Step 4. (update and repeat)}: Set $x^{k+1}=x^k+t_kd^k$. Update $k \leftarrow k+1$ and go to Step~1.\\

  \end{algorithmic}
\end{algorithm}

The linear system \eqref{newton} gives the standard Newton direction for the KKT system. The direction is re-scaled to avoid a direction with norm greater than $100$. The minimization of $L(x^k+td^k)$ is done by a sequence of straightforward minimization of (smooth) unidimensional convex quadratics. The safeguarded projection is considered to avoid numerical errors when the Newton direction is not accurately computed.

%the (smooth) quadratic $$\tilde L(t)=\frac{1}{2}(x^k+td_x)^TQ(x^k+td_x)+c^T(x^k+td_x)+\frac{\rho}{2}\left[\sum_{x_i^k\leq l_i}(x_i^k+t[d_x]_i-l_i)^2+\sum_{x_i^k\geq u_i}(x_i^k+t[d_x]_i-u_i)^2\right],$$

\begin{color}{red}proof of theorems...\end{color}

Moreover, the stopping precision $\epsilon_k$ will also be denoted by 
$\epsilon$.

In the following, we consider $x^k$ as the point obtained in the
$k$-th iteration of the algorithm for solving the subproblem, thus
not associated to the point $x^k$ generated by the external
algorithm.

For a point $x^k$,  the gradient of $L$ at $x^k$ is $\nabla
L(x^k)=c+\rho \left(-\max\left\{l-x^k,0\right\}+\max\left\{x^k-u,0\right\}\right)$. 
Moreover, we define $I^k_{int}=\{i: \; l_i<x^k_i<u_i \}$, 
$I^k_{out}=\{1,2,\cdots,n\}\backslash I^k_{int}$ and, for $k \geq 1$,  $I^k_{change}=\{i \in I^{k-1}_{int} \cap I^k_{out} \} $. 
Define $H^k \in \R^{n \times n}$ 
as the diagonal matrix such that $H^k_{ii}=\rho$ if $i \in I^k_{out}$.

The algorithm to solve the subproblem \eqref{subprobLA} is stated as follows.\\


\begin{algorithm}[H]
  \caption{Subproblem algorithm}
  \label{algsubprob}
  \begin{algorithmic}
\State \noindent  \textbf{Step 0.} \textit{Initialization}\\
 As initial approximation we choose, arbitrarily,
 $x^0$ such that $Ax^0=b$. Set $\epsilon_{reg}>0$, $\alpha \in (0,1)$, and
 $0<\gamma_{\min} \leq \gamma_{\max}<1$, $skipreg \in \{0,1\}$, $prematurestop=0$.\\

\State \noindent \textbf{Step 1.}   If $x^k$ is a exact solution of \eqref{subprobLA}, stop. 
If $checkpremstop=1$, $k \geq 1$, $I^k_{change}\neq \emptyset$ and $x^k$ is an approximated solution of \eqref{subprobLA}
define $prematurestop=1$.\\


\State \noindent \textbf{Step 2.}  If $skipreg=0$ set $\sigma_k=\epsilon_{reg}$.
Otherwise, compute $\sigma_k \in
\{0,\epsilon_{reg}\}$ as the minimum value such that, for all non null $d
\in \R^n$ in the null-space of $A$, we have that $d^T
[H^k + \sigma_k I] d > 0 $.\\

    \State  \noindent \textbf{Step 3.} Take $d^k$ as the solution of the Quadratic Programming problem
given by
\begin{equation} \label{qpp}
  \mbox{Minimize } \frac{1}{2} d^T
  [H^k + \sigma_k I] d + \nabla L(x^k)^T d
  \mbox{ subject to } A d=0.
\end{equation}

    \State  \noindent \textbf{Step 4.} Set $t_k=1$. While
\begin{equation} \label{lopez}
L(x^k+t_k d^k) \leq L(x^k) + \alpha t_k \nabla L(x^k)^T d^k,
\end{equation}
is not satisfied, compute $t_k \in [\gamma_{\min}t_k, \gamma_{\max}t_k]$.


    \State  \noindent \textbf{Step 5.} If $skipreg=0$, set $skipreg \in \{0,1\}$. Set
\begin{equation}\label{xkmas}
 x^{k+1} = x^k + t_k d^k,
\end{equation}
update $k \leftarrow k+1$ and go to Step~1.\\

  \end{algorithmic}
\end{algorithm}


In Step 4, if $t_k=1$ is rejected, we define $I^k_{changel}=\{i; \; l_i<x^k_i<u_i \text{ but } [x^k+d^k]_i<l_i\}$, 
$I^k_{changeu}=\{i; \; l_i<x^k_i<u_i \text{ but } [x^k+d^k]_i>u_i \}$, $I^k_{change}=I^k_{changel} \cup  I^k_{changeu}$
$s^k_l=\min_{i \in I^k_{changel}} \{\frac{l_i-x^k_i}{d^k_i}\}$,
$s^k_u=\min_{i \in I^k_{changeu}} \{\frac{u_i-x^k_i}{d^k_i}\}$, $s^k=\min\{s^k_l,s^k_u\}$
and the first new attempt of  $t_k$ as $\min\{\gamma_{\max},\max\{s^k,\gamma_{\min} \}\}$. 

Once again, in order to analise the asymptotic behaviour, we will consider that the Algorithm 
\ref{algsubprob} stops only if there is $k_0$ such that $x^{k_0}$ is an exact
solution of \eqref{subprobLA}. In this case, we declare that  $x^{k}=x^{k_0}$ for all $k \geq k_0$.


Once $H_k \geq 0$, and Step 3 is a standard Armijo rule line search
for a differentiable function with a descent direction $d^k$, the algorithm
is well defined whenever $F \neq \emptyset$.



Let $Z$ be a matrix such that its columns form an orthonormal basis
of the null-space of $A$. Since the number of possible $H_k$ is
finite, we have that the eigenvalues of $Z^T [H_k+\sigma_k I] Z$ lie
in a positive interval $[\sigma_{\min}, \sigma_{\max}]$. Let $\sigma:=
\min \left\{\frac{\sigma_{\min}}{2}, \frac{1}{2+\sigma_{\max}}
\right\}$.

Since $d=0$ is feasible to (\ref{qpp}),  we have that
$$ \nabla L(x^k)^T d^k + \frac{1}{2}(d^k)^T [H_k+\sigma_k I] d^k \leq 0.$$
Therefore,
\begin{equation} \label{condangsuf}
 \nabla L(x^k)^T d^k \leq -\frac{\sigma_{\min}}{2} \|d^k\|^2 \leq - \sigma \|d^k\|^2.
 \end{equation}
Condition \eqref{condangsuf} ensures that $d^k$ satisfies a
sufficient descent criterion.


Since $d^k$ is the solution of the linearly constrained problem
(\ref{qpp}),
$$P_{N(A)}(d^k-[H_k+\sigma_k I]d^k-\nabla L(x^k))-d^k=0.$$
Changing variables ($x=x+d$) and using the fact that the projections
are non-expansive we have:
\[
\|P_F (x^k - \nabla L(x^k))-x^k\|=\|P_{N(A)} (-\nabla L(x^k))\|
\]
\[
= \|P_{N(A)} (-\nabla L(x^k))-P_{N(A)}(d^k-[H_k+\sigma_k
I]d^k-\nabla L(x^k)) +P_{N(A)}(d^k-[H_k+\sigma_k I]d^k-\nabla
L(x^k))\|
\]
\[
\leq \|P_{N(A)} (-\nabla L(x^k))-P_{N(A)}(d^k-[H_k+\sigma_k
I]d^k-\nabla L(x^k))\| +\|P_{N(A)}(d^k-[H_k+\sigma_k I]d^k-\nabla
L(x^k))\|
\]
\[
\leq \|d^k-[H_k+\sigma_k I]d^k\|+\|d^k\| \leq (2+\sigma_{\max})
\|d^k\|.
\]
So, we have that
\begin{equation} \label{bronte}
\sigma \|P_{F}(x^k - \nabla L(x^k))-x^k\| \leq  \|d^k\|.
\end{equation}
Condition \eqref{bronte} shows that small steps are allowed only if
$x^k$ is close to a solution of \eqref{subprobLA}.

The next Lemma shows that the Armijo linesearch can be completed
with $t_k$ bounded away from zero.
\begin{lemma} \label{tlonge0}
  \emph{There exists $\bar{t}>0$ such that $t_k \geq \bar{t}$ for all $k \in
\N$.}
\end{lemma}

\noindent {\it Proof.} Since $\rho$ is greater than all the eingvalues of $H^k$, it is straightforward that
\begin{equation}\label{limitationf}
L(x^k+td^k) \leq L(x^k)+t \nabla L(x^k)^Td^k+ \rho t^2 \|d^k\|^2 /
2.
\end{equation}

If $t \leq 2(1-\alpha) \sigma / \rho$, by \eqref{limitationf}, and
\eqref{condangsuf}  we have that
\begin{eqnarray*}
L(x^k+td^k) &\leq& L(x^k)+t \left( \nabla L(x^k)^Td^k+ (1-\alpha)\sigma \|d^k\|^2 \right)\\
&\leq& L(x^k)+t\left(\nabla L(x^k)^Td^k-(1-\alpha)\nabla L(x^k)^Td^k\right)\\
&\leq& L(x^k)+t \alpha \nabla L(x^k)^Td^k.
\end{eqnarray*}
Thus we have that condition \eqref{lopez} is satisfied with $t^k$
bounded way from zero. \halmos



\begin{theorem} \label{ptolimitesolucao}
  \emph{All the limit points of $\{x^k\}$ are solutions of problem
  \eqref{subprobLA}.}
\end{theorem}

\noindent {\it Proof.} By \eqref{condangsuf}, \eqref{lopez} and Lemma
\ref{tlonge0}, there exists $\bar{t}>0$ such that
\begin{eqnarray*}
L(x^k+t_k d^k)
&\leq& L(x^k) + \alpha t_k \nabla L(x^k)^T d^k\\
&\leq&  L(x^k)- t_k \alpha \sigma \|d^k\|^2\\
 &\leq&  L(x^k)- \bar{t} \alpha \sigma \|d^k\|^2.
\end{eqnarray*}
 Thus
$$L(x^{l+1})-L(x^0)=
\sum_{k=0}^l(L(x^{k+1})-L(x^k))$$
$$\le -\alpha \sigma \bar{t}\sum_{k=0}^l\|d^k\|^2.$$


By Lemma \ref{Lvaiinfinitosexvai}, $\{L(x^k)\}$ is bounded below, so
the series $\sum_{k=0}^\infty \|d^k\|^2$ is convergent, and thus,
$\{\|d^k\|\}$ converges to zero.

By \eqref{bronte} we have that, if $x^*$ is a limit point $\{x^k\}$,
it satisfies the L-AGP optimality condition \cite{ahm}. Since the
constraints of \eqref{subprobLA} are linear, we have that $x^*$ is a
stationary point for \eqref{subprobLA}. Moreover, due to the
convexity of \eqref{subprobLA}, we have that $x^*$ is a solution of
\eqref{subprobLA}. \halmos

Finally, the next result shows that the subproblem is solved by
Algorithm \ref{algsubprob}.

\begin{cor} \label{achsolsubprob}
  \emph{One of the solutions of problem
  \eqref{subprobLA} is a limit point of $\{x^k\}$.}
\end{cor}



By Lemma \ref{Lvaiinfinitosexvai} we have that $\lim_{\|x\| \to
\infty} L(x) =  \infty$, so, by \eqref{lopez}, the sequence
generated by Algorithm \ref{algsubprob} remain in a compact set.
Thus, there exists a limit point of $\{x^k\}$, which, by Lemma
\ref{ptolimitesolucao}, is a solution of problem
  \eqref{subprobLA}. \halmos

  
  \begin{cor} \label{convsolunicasubprob}
  \emph{If the problem
  \eqref{subprobLA} has an unique solution then the sequence
  $\{x^k\}$ converges to the solution.}
\end{cor}
\noindent {\it Proof}  By \ref{Lvaiinfinitosexvai} and \eqref{lopez} the sequence
$\{x^k\}$  remain in a compact set. By Theorem \ref{ptolimitesolucao}, we have that all the limit
points are solutions of problem   \eqref{subprobLA}, so the hole sequence converges to the solution. \halmos


The next results discuss the finite convergence of the subproblems.


\begin{theorem}\label{convfinitasubprob}
 Supose that $\alpha \leq \frac{1}{2}$, the subproblem \eqref{subprobLA} has a unique solution $x^*$ and that
 $l_i \neq x^*_i \neq u_i$ for all $i$,
 then there exists $k_1$ such that $x^{k_1}=x^*$.
\end{theorem}

\noindent {\it Proof}  Since $l_i \neq x^*_i \neq u_i$ for al $i$, $L(x)$ is a smooth quadratic function $q(x)$ in a neiborhood of
$x^*$. 
Since \eqref{subprobLA} has an unique solution, $x^*$ is also the unique solution of the quadratic problem 
$\text{minimize } q(x) \text{ subject to } Ax=b$. Thus, we have that $d^T\nabla^2 L(x)d>0$ for all nonull $d$ such that
$Ad=0$ and $x$ is close to $x^*$. 
By \ref{convsolunica} we have that the whole sequence $\{x^k\}$ converges to $x^*$, so for $k$ large enough 
$H^k+\sigma_k I=\nabla^2 L(x^k)$. Thus, for $k$ large enough, $d^k$ is the Newton step for a strictly convex quadratic
problem, and so $x^k+d^k$ is the solution of \eqref{subprobLA}. 

Since $x^*$ is a solution of \eqref{subprobLA}, we have that $\nabla q(x^*)$ is ortogonal to every $d$ in the kernel of $A$.
Moreover, the Taylor series of $q(x)$ is finite and $\nabla q(x)= \nabla  q(x^*)+ \nabla^2 q(x^*) (x-x^*)$.
So, considering that $\alpha \leq \frac{1}{2}$ and $x^k$ close to $x^*$,   we have
\begin{eqnarray*}
L(x^*)
&\leq& L(x^k) -  \nabla L(x^*)^T (x^k-x^*)-\frac{1}{2}(x^k-x^*)^T\nabla^2 L(x^*)(x^k-x^*)\\
&\leq&  L(x^k)- \frac{1}{2}\nabla L(x^*)^T (x^k-x^*)-\frac{1}{2}\nabla L(x)^T(x^k-x^*)\\
 &\leq&  L(x^k)+\frac{1}{2}\nabla L(x)^T d^k\\
  &\leq&  L(x^k)+\alpha \nabla L(x)^T d^k.
\end{eqnarray*}
Thus, by \eqref{lopez}, we have that $x^{k+1}=x^*$. \halmos


\begin{theorem} \label{teoconvfinitasuprob2}
Supose that $\alpha \leq \frac{1}{2}$, the original problem \eqref{PL} has a unique solution $x^*$, the regularity and the strict complementarity
holds on $x^*$ and that $\mu_l^* < \mu_{\max}$ and $\mu_u^* < \mu_{\max}$. So, for $k$ large enough, Algorithm
\ref{algsubprob} finds the exact solution of the subproblems \eqref{subprobLA} in finite time.
\end{theorem}

\noindent {\it Proof} By Corollary \ref{convsolunica} we have that the sequence $\{x^k\}$, generated by the Agorithm 
\ref{ALalg}, converges to $x^*$. Moreover, by  Lemmas 7.1 and 7.2  of \cite{bmbook},
we have that $\lim \mu^k_l=\mu^*_l$ and $\lim \mu^k_u=\mu^*_u$.

By the regularity we have that, for $k$ large enough, subproblem \eqref{subprobLA} has unique solution.
Let us denote by $\bar{x}^k$ the solution of the subproblem \eqref{subprobLA} at the iteration $k$.
By Lemma \ref{LemaCompcertas}, we have that, for $k$ large enough, 
$$l_i+\frac{[\mu^k_l]_i}{\rho_k} \neq x_i \neq u_i-\frac{[\mu^k_u]_i}{\rho_k}$$ 
for all $i$. So, by Theorem \ref{convfinitasubprob}, we have that the exact solution of the subproblems 
\eqref{subprobLA} in finite time.
\halmos


\begin{cor}
 Suppose that $\alpha \leq \frac{1}{2}$, the original problem \eqref{PL} has a unique solution $x^*$, the regularity and the strict complementarity
holds on $x^*$ and that $\mu_l^* < \mu_{\max}$ and $\mu_u^* < \mu_{\max}$. So, the sequence $\{x^k\}$  generated by Algorithm 
\ref{ALalg} employing Algorithm \ref{algsubprob} to solve the subproblems, converges to $x^*$ in finite time.
\end{cor}

\noindent {\it Proof.} By Theorem \ref{teoconvfinitasuprob2} we have that the subproblems are exactly solved for $k$ large enough.
So, by Corollary \ref{convfinitasolunica}, we have that the Algorithm finds $x^*$ in finite time. \halmos

\section{Numerical experiments}
\graphicspath{ {./figures/} }

The goal of our numerical experiments is to show that the augmented Lagrangian method proposed is compatible with a simple implementation of a pure interior point method. This is done in order to motivate further studies on augmented Lagrangian methods applied to linear and convex problems. We implemented Algorithm \ref{ALalg} (called \texttt{LAQP}) and the interior point method for convex quadratic programming as described in~\cite{gondzio25} (called \texttt{IPM}), both in Julia~\cite{Bezanson:2017g}. In \texttt{IPM}, finite upper bounds were included as new constraints by adding slack variables. We used subroutine MA57~\cite{Duff:2004cx} from HSL~\cite{HSL:7NBcL3Rm} to solve the augmented linear systems that arise in both methods. We run the tests on an iMac Pro with Intel Xeon W 3.2 GHz Processor and 32 GB RAM.
 
Our tests are based on the \textsc{Netlib}~\cite{Dongarra:1987jk} test collection of linear programming problems. Due to the difficulty of the problems, we use a presolved version of the collection available at~\cite{HagerCoapSoftware}. Our main goal is to apply the algorithm to linear programming problems, however, we found out that our implementation is too sensible to a fine tuning of the regularization parameter and the results were not satisfatory. Thus, we added a homogeneous quadratic term to the objective function with Hessian equal to the identity matrix, in order to obtain strictly convex separable quadratic problems. In this way, we may take the regularization parameter equal to zero at each iteration and we may use the second-order Lagrange multiplier update, which is easily computable. Also, the Newtonian direction can be computed without refined linear solvers.

We set the initial point using two steps. In the first step, we choose one of the four possible initial point candidates:

\begin{description}
  \item[\texttt{New}:] in this approach, $x^0$ is computed as the projection of the solution of  $\min \frac{1}{2}x^{T}Qx + c^Tx $, subject to $Ax=b$, onto the box constraints. That is, a solution of \eqref{PL} ignoring the box constraints is computed and then projected onto the box constraints. Next, $\lambda^{0}$ is computed as the least squares solution of $A^{T}\lambda = -(Qx^{0}+c)$, while  $\mu_l^0=-\min\{0, \mu\}$  and
$\mu_u^0=\max\{0,\mu\}$, with $\mu = -(Qx^{0}+c+A^{T}\lambda^{0})$;


\item [\texttt{Ones}:] we define $x^{0}$ and $\mu_l^0$ as the vector of ones and $\lambda^{0}$ and $\mu_u^0$ as the vector of zeros;

\item [\texttt{Mehrotra}:] $x^{0}$, $\lambda^{0}$, $\mu_l^0$ and $\mu_u^0$ are computed by the usual Mehrotra-type heuristic~\cite{Mehrotra:1992wr};

\item [\texttt{OnesM}:] we define $x^{0}$ as the vector of ones, $\mu_l^0 = Qx^{0}+c +\beta I$, where $\beta\geq0$ is chosen such that all components of $\mu_l^0$ are at least $\num{0.1}$,  $\lambda^{0}$ is computed as the least squares solution of $A^{T}\lambda = -(Qx^{0}+c-\mu_l^0)$,  and $\mu_u^0$ is the vector of zeros.


\end{description}

Independently of the choice above, the second step depends on the method: for \texttt{LAQP}, we perform a full Newton step to recover feasibility regarding $Ax=b$; for  \texttt{IPM}, we use the shifting strategy implemented by Mehrotra~\cite{Mehrotra:1992wr} to assure that the initial point is (sufficiently) interior with respect to the box constraints. 

We start by considering 62 (out of 95) \textsc{Netlib} problems with less than \num{10000} non-zero elements in the matrix of constraints\footnote{Problems \texttt{forplan, gfrd-pnc} and \texttt{pilot.we} were removed from the \textsc{Netlib} collection, since they were not available in the presolved library \cite{HagerCoapSoftware}.} in order to decide  which of those aforementioned four possible initial points provide better numerical results, for each method.

With respect to the choice of Lagrange multiplier update for \texttt{LAQP}, we use a \texttt{Hybrid} approach, which uses the second-order update only when the solution of the subproblem is within the same region as the current iterate, in the sense that the corresponding Hessians of the augmented Lagrangian function coincide. Otherwise, the first-order update is used. The performance profiles, computed in terms of the number of linear systems solved, are presented below on \Cref{fig:PP-4points}, where for each algorithm all initial points are compared.

%All initial points LAQP and IPM on the SMALL subset of \textsc{Netlib} (with Hybrid update?).
\begin{figure}[!ht]\centering
\begin{subfigure}[t]{.72\textwidth}
\centering
\includegraphics[width=\textwidth]{PP_IPM-4points}
   \caption{}
  \label{fig:PP_IPM-4points}
  
\end{subfigure} 
\\
   % \hspace*{.21in}  
   \begin{subfigure}[t]{.72\textwidth}
\centering
\includegraphics[width=\textwidth]{PP_LAQPHybrid-4points}
   \caption{}
  \label{fig:PP_LAQPHybrid-4points}

\end{subfigure}

\caption{Performance profiles of \texttt{IPM} and \texttt{LAQP-Hybrid} with each of the four initial points.}\label{fig:PP-4points}
\end{figure}



Based on these results, we decide to use \texttt{Mehrotra}  as the initial point for \texttt{IPM} and \texttt{New} for \texttt{LAQP}. The performance profile comparing those two results is presented in \Cref{fig:PP_LAQPHybridNew-IPMMehrotra}.

%The same test on the SMALL subset{}

\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{PP_LAQPHybridNew-IPMMehrotra}
\caption{Performance profiles of \texttt{IPM-Mehrotra} and \texttt{LAQPHybrid-New}.}\label{fig:PP_LAQPHybridNew-IPMMehrotra}
\end{figure}


With the same two configurations, we perform a more thorough test on all 95 \textsc{Netlib} problems, modified in the same way as describe before. The corresponding performance profile is on \Cref{fig:PP_LAQPNew-IPMMehrotra_All}.

%NEW initial point LAQP vs MEHROTRA IPM on all \textsc{Netlib}
\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{PP_LAQPNew-IPMMehrotra_All}
\caption{Performance profiles of \texttt{IPM-Mehrotra} and \texttt{LAQPHybrid-New} for all 95 \textsc{Netlib} problems.}\label{fig:PP_LAQPNew-IPMMehrotra_All}
\end{figure}


We emphasize that our goal is to compare a pure interior point method versus the pure augmented Lagrangian method proposed, without sophisticated accelerations and safeguards. This explains the overall poor behavior of both methods. As pointed out in~\cite{Haeser:2017wl}, IPOPT~\cite{Wachter:2005hk}, a well tuned interior point method for nonlinear optimization, solves 56 of the 68 smallest problems of the \textsc{Netlib} collection when limited to 300 iterations. They also show that no strict interior point exists on circa 65\% of \textsc{Netlib} problems, which indicates that those  are rather difficult problems and, in particular, explains the overall poor behavior of the pure interior point implementation.


To analyze the behavior of both methods more carefully, \Cref{fig:Bar_LAQPNew-IPMMehrotra_All_2} below represents a ratio calculated for each of the 95 available problems on the horizontal axis, ordered in increasing size of non-zero entries in the constraint matrix. A bar with a positive vertical coordinate indicates that \texttt{LAQPHybrid-New} is the best solver in terms of the number of linear systems solved, while a negative coordinate indicates that \texttt{IPM-Mehrotra} is better. The size of the bar corresponds to the absolute value of $r_i$, for each problem $i$, computed as 
\[
r_i = \frac{\#S_{\mathtt{IPM-Mehrotra}}^{i} - \#S_{\mathtt{LAQPHybrid-New}}^{i} }
{\max\{ \#S_{\mathtt{IPM-Mehrotra}}^{i} , \#S_{\mathtt{LAQPHybrid-New}}^{i} \}},
\] where $\#S_{j}^{i}$ is the number of linear systems used by solver $j$ to solve problem $i$. For instance, if $r_i = 0.8$ that means \texttt{LAQPHybrid-New} was 80\% better, in the sense that \texttt{IPM-Mehrotra} solved $5$ times more linear systems than \texttt{LAQPHybrid-New} on problem $i$ and the bar points upwards, while $r_i = -0.3$ indicates that \texttt{IPM-Mehrotra} was 30\% better on problem $i$, i.e., \texttt{LAQPHybrid-New} solved $10/7$ times more linear systems than \texttt{IPM-Mehrotra} and the bar points downwards. If only one solver found a solution, $r_i$ is either $1.0$ or $-1.0$, and a red cross is marked when none of the solvers found a solution. 

% BAR graphic
\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{Bar_LAQPNew-IPMMehrotra_All_2}
\caption{Ratio of performance between \texttt{LAQPHybrid-New} and \texttt{IPM-Mehrotra}.}
\label{fig:Bar_LAQPNew-IPMMehrotra_All_2}
\end{figure}

%with the NEW initial point on LAQP, check that the second-order update is indeed better than the first-order one

We now verify, on \Cref{fig:PP_LAQP_Dual-Linear-Hybrid}, the efficacy of the \texttt{Hybrid} strategy for updating the Lagrange multipliers. We compare it with the \texttt{Linear} update, where the first-order update is used in every iteration, and the \texttt{Dual} update, where the second-order update is always used.
\begin{figure}[!ht]\centering
\includegraphics[width=.72\textwidth]{PP_LAQP_Dual-Linear-Hybrid}
\caption{Performance profile for  \texttt{LAQPHybrid-New}, \texttt{LAQPLinear-New} and   \texttt{LAQPDual-New}.}
\label{fig:PP_LAQP_Dual-Linear-Hybrid}
\end{figure}
Based on these results, we conclude that the augmented Lagrangian method with the \texttt{Hybrid} Lagrange multiplier update and the \texttt{New} initial point had the best performance in our tests.

% \begin{color}{red}Rejected pictures:\end{color}

% \includegraphics[scale=0.3]{PP_LAQP_Linear-Hybrid}

% \includegraphics[scale=0.3]{PP_LAQPLinear-4points}

% \includegraphics[scale=0.3]{PP_LAQP-4points}


We envision that the main quality of our method, in comparison with interior point methods, is the possibility of naturally exploiting a very good initial point, without the need of projecting onto a strict interior. Hence, we consider the previous test of \texttt{LAQPHybrid-New} and \texttt{IPM-Mehrotra} on the whole (modified) \textsc{Netlib} test set and we select only the $29$ problems where both solvers were successful. We then take a perturbation of the solution $(x^*,\lambda^*,\mu_l^*,\mu_u^*)$ found by each solver and we run each solver again with this point as the initial point. The perturbation for the augmented Lagrangian is given by $[x^0]_i=(1+10^{-3}n_i)[x^*]_i+m_i$, where $n_i$ is sampled from the standard normal distribution and $m_i$ is a random value between $-10^{-5}$ and $10^{-5}$, for each coordinate $i$. A similar perturbation is performed for $\lambda^0, \mu_l^0$ and $\mu_u^0$. For the interior point method, after the perturbation of $(x^*,\lambda^*,\mu_l^*,\mu_u^*)$ as described above, we perform a Mehrotra type shift to obtain an interior initial point and its corresponding multipliers.  The results are on \Cref{tab:LAQP_IPM_Perturbed_x} where the first column is the name of the problem on the \textsc{Netlib} collection, the second column is the number of non-zero entries of the matrix $A$, the third and forth columns are the number of iterations of \texttt{LAQP} and \texttt{IPM}, respectively, needed for solving the problem originally from their corresponding initial points, while the fifth and sixth columns correspond to the number of additional iterations for \texttt{LAQP} and \texttt{IPM}, respectively, needed for solving the problem from a perturbation of the solution.

\begin{table}[!hbt]\centering
\caption{Number of linear systems solved before and after starting from a perturbed solution for \texttt{LAQP} and \texttt{IPM}.}
\label{tab:LAQP_IPM_Perturbed_x}\small
\begin{tabular}{>{\ttfamily}lS[table-format=5,group-minimum-digits=3]cccc}
\toprule
    & &  \multicolumn{2}{c}{First run} &   \multicolumn{2}{c}{Second run}  \\
{\normalfont \textbf{Problem}}  & {\textbf{Nonzeros}} &\texttt{LAQP} &   \texttt{IPM} & \texttt{LAQP}  & \texttt{IPM} \\ 
\midrule  
afiro    & 88    &   16   &  13   &     2   &    3 \\ 
adlittle & 465   &   61   &  17   &     2   &    5 \\ 
agg2     & 4515  &   64   &  34   &     2   &    7 \\ 
agg3     & 4531  &   61   &  33   &     2   &    6 \\ 
bandm    & 2659  &  405   & 146   &   148   &    8 \\ 
beaconfd & 3476  &    2   &  15   &     2   &    5 \\ 
blend    & 521   &   22   &  16   &     2   &    4 \\ 
bore3d   & 1525  &   36   &  38   &    20   &    7 \\ 
brandy   & 2150  &   91   & 167   &   --    &    6 \\ 
capri    & 1786  &  237   & 291   &    70   &   10 \\ 
e226     & 2767  &   73   &  28   &    77   &    6 \\ 
ganges   & 7021  &  152   &  28   &    24   &    8 \\ 
grow15   & 5665  &   76   &  79   &     2   &   43 \\ 
grow22   & 8318  &   40   &  84   &     2   &   48 \\ 
grow7    & 2633  &   71   &  80   &     5   &   44 \\ 
israel   & 2358  &  179   & 132   &   115   &    9 \\ 
kb2      & 291   &  108   &  20   &    89   &    6 \\ 
lotfi    & 1086  &   14   &  17   &     2   &    5 \\ 
modszk1  & 4158  &  294   &  50   &   --    &    6 \\ 
qap8     & 8304  &  131   &  16   &   --    &    4 \\ 
recipe   & 752   &    2   &  31   &     7   &   15 \\ 
sc105    & 281   &    8   &  21   &     2   &    3 \\ 
sc205    & 552   &   16   &  62   &     2   &    4 \\ 
sc50a    & 131   &    2   &   7   &     2   &    3 \\ 
sc50b    & 119   &    4   &  10   &     2   &    3 \\ 
scagr25  & 2029  &   73   &  34   &     6   &    6 \\ 
scagr7   & 553   &   52   &  19   &     2   &    4 \\ 
scfxm1   & 2612  &  122   &  76   &    27   &    8 \\ 
scfxm2   & 5229  &  207   & 193   &    86   &    8 \\ 
scorpion & 1708  &  102   &  28   &    45   &    8 \\ 
scrs8    & 4029  &  195   &  31   &   195   &    6 \\ 
scsd1    & 3148  &   42   &  93   &    56   &   47 \\ 
scsd6    & 5666  &   58   &  92   &     5   &   46 \\ 
sctap1   & 2052  &   69   &  45   &     3   &    7 \\ 
sctap2   & 8124  &  126   &  38   &    30   &    7 \\ 
seba     & 4874  &    9   &  12   &     2   &    3 \\ 
share2b  & 730   &   43   &  37   &     2   &   10 \\ 
shell    & 4900  &  131   &  36   &   129   &    6 \\ 
ship04s  & 5810  &  150   &  26   &    28   &    5 \\ 
stair    & 3857  &   81   &  59   &     5   &    8 \\ 
standata & 3038  &   74   &  37   &     7   &    5 \\ 
standgub & 3147  &   74   &  37   &    38   &    6 \\ 
standmps & 3686  &  121   &  51   &    33   &   17 \\ 
stocfor1 & 474   &   15   &  13   &     5   &    4 \\ 
stocfor2 & 9492  &   70   &  52   &    12   &    6 \\ 
vtp-base & 4523  &   38   &  33   &     2   &    5  \\ 
\bottomrule
\end{tabular}
\end{table}

The \texttt{LAQPHybrid} variant of \texttt{LAQP} always performs the first Lagrange multiplier update as the first-order one, which we have observed that it is not efficient for solving the problem from a perturbed solution. Hence, in this test, we always perform the second-order update, that is, \texttt{LAQPDual} is run. We note that \texttt{LAQP} was not able to solve problem \texttt{qap8} from the perturbed solution.

In \Cref{tab:LAQP_IPM_Perturbed_x} we can see that in $18$ problems ($62$\%), \texttt{LAQP} found the solution faster than \texttt{IPM}. In this cases, \texttt{LAQP} took in average $2.2$ additional iterations, while \texttt{IPM} took, in average, $5.1$ iterations, excluding three cases where more than $40$ additional iterations were necessary. In the remaining $11$ problems ($38$\%), where \texttt{IPM} found the solution faster than \texttt{LAQP}, we can see that, in general, these were cases where in the original run \texttt{LAQP} was already inferior to \texttt{IPM}. More specifically, in $12$ problems where \texttt{LAQP} was originally faster than \texttt{IPM}, \texttt{LAQP} was still faster to recover the solution on the second run on $10$ problems ($83$\%), while on $17$ problems where \texttt{IPM} was faster on the first run, \texttt{IPM} was able to recover the solution on the second run faster on $9$ problems ($53$\%).

We consider again this collection of $29$ problems where both methods found a solution and their corresponding solutions found, say, $x^*$. We now perform two tests by using the solutions found as initial points to a perturbed problem. The first test consists of perturbing the problem in the following way: We find the first index $i$ of $x^*$ such that $l_i=[x^*]_i$ and we then redefine $l_i\leftarrow l_i+1$. This corresponds to a sensibility analysis perturbation. Now $x^*$ is infeasible but it can be used as the initial point for \texttt{LAQP}. For \texttt{IPM} we use $x^*$ and the corresponding Lagrange multipliers followed by a Mehrotra shift as the initial point, to recover interiority. The second test consists of finding the index of the largest interior component of $x^*$, that is, $l_i<[x^*]_i<u_i$. For this index we re-define $u_i\leftarrow [x^*]_i-1$. Since in our implementation of \texttt{IPM} upper bounds are treated explicitly by adding slack variables, there is no need to recover interiority and we may use $x^*$ and the corresponding Lagrange multipliers as the initial point. In both tests we consider appropriate safeguards to ensure $l\leq u$. The results are reported on \Cref{tab:LAQP_IPM_Perturbed_l}, where a dash corresponds to a solver not being able to solve the problem.

\begin{table}[!hbt]\centering
\caption{Number of systems solved after perturbing the problem with respect to an active bound (Test 1) and an inactive bound (Test 2).}
\label{tab:LAQP_IPM_Perturbed_l}
\begin{tabular}{>{\ttfamily}lcc|cc}
\toprule
    &  \multicolumn{2}{c}{Test 1} &   \multicolumn{2}{c}{Test 2}  \\
    \midrule  
{\normalfont \textbf{Problem}} & \texttt{LAQP}   & \texttt{IPM} & \texttt{LAQP}   & \texttt{IPM} \\ 
 afiro    &     7 &    8   &      2  &     6        \\
 adlittle &     2 &    5   &      2  &     6        \\
 agg2     &     2 &    4   &      2  &     8        \\
 agg3     &     2 &    4   &      2  &     6        \\
 bandm    &     8 &    4   &    --   &    81        \\
 beaconfd &     2 &    3   &      2  &     5        \\
 blend    &   137 &  233   &      7  &    10        \\
 bore3d   &    10 &    9   &      2  &     8        \\
 brandy   &     4 &    8   &      2  &     6        \\
 capri    &    16 &    9   &      2  &    16        \\
 e226     &   --  &  -50   &      2  &     6        \\
 ganges   &    75 &    4   &     35  &     6        \\
 grow15   &     5 &   16   &      6  &    39        \\
 grow22   &     5 &   24   &      9  &   154        \\
 grow7    &    51 &   35   &      4  &    24        \\
 israel   &    67 &   10   &      6  &    13        \\
 kb2      &    42 &  114   &     48  &    15        \\
 lotfi    &     2 &    3   &      2  &     5        \\
 modszk1  &   417 &    5   &    --   &     6        \\
 qap8     &   --  &  -50   &    --   &   138        \\
 recipe   &    28 &   38   &    --   &    20        \\
 sc105    &     2 &    4   &      2  &     7        \\
 sc205    &     2 &    4   &      2  &     5        \\
 sc50a    &     2 &   -5   &      2  &     6        \\
 sc50b    &     2 &    4   &      3  &     9        \\
 scagr25  &     2 &    4   &      2  &     9        \\
 scagr7   &     2 &    4   &      2  &     9        \\
 scfxm1   &     4 &    9   &      7  &    14        \\
 scfxm2   &    49 &    8   &    --   &    25        \\
 scorpion &    36 &  360   &     35  &     9        \\
 scrs8    &     2 &  -50   &      2  &     6        \\
 scsd1    &    22 &   12   &     19  &    57        \\
 scsd6    &   106 &   42   &    135  &    99        \\
 sctap1   &    25 &   59   &      3  &     9        \\
 sctap2   &    36 &  206   &      6  &    13        \\
 seba     &     2 &    4   &      2  &     8        \\
 share2b  &    24 &   80   &     24  &    15        \\
 shell    &    15 &    4   &    --   &    22        \\
 ship04s  &    32 &    5   &      5  &     7        \\
 stair    &    11 &   45   &     16  &     7        \\
 standata &   391 &   18   &     34  &    29        \\
 standgub &   391 &   18   &     34  &    29        \\
 standmps &   124 &   42   &    104  &    29        \\
 stocfor1 &    61 &   12   &     24  &    20        \\
 stocfor2 &   270 &  274   &    --   &   152        \\
 vtp-base &     2 &    4   &      2  &     6        \\ 
\bottomrule
\end{tabular}
\end{table}

In the first test, excluding two problems where no method found a solution (possibly due to it having become infeasible), \texttt{LAQP} was faster in $20$ problems ($74$\%) while \texttt{IPM} was faster in $7$ problems ($26$\%). On the second test, \texttt{LAQP} was faster in $21$ problems ($72$\%) while \texttt{IPM} was faster in $8$ problems ($28$\%).

\section{Conclusions}


%Neste artigo quebramos o paradigma de resolver problemas de
%Programação Linear usando métodos de restrições ativas do tipo
%Simplex ou métodos do tipo Pontos interiores. Propusemos a ideia de
%usar uma abordagem do tipo Lagrangiano Aumentado para resolver este
%problema e discutimos diversas vantagens desta alternativa em
%relação a métodos do tipo Pontos Interiores.
%
%Com nossa proposta abrimos um leque de possibilidades de
%investigação sobre o tema. O estudo da convergência finita do
%algoritmo exterior, e a convergência finita sem busca linear no
%algoritmo interno são questões importantes que devem ser estudadas
%em pesquisas futuras.
%
%Boas estimativas do ponto inicial e dos multiplicadores de Lagrange podem fazer o método muito eficiente. 
%Acreditamos que isso possa ser usado em métodos de programação linear ou quadrática sequencial
%para programação não linear. O ponto inicial pooderia ser muito melhor aproveitado do que em métodos
%de pontos interiores ou no método simplex.

In this paper we considered an implementation of a penalty method for convex quadratic programming, where bound constraints are penalized and linear equality constraints are carried out to the subproblems. For solving the piecewise quadratic subproblems we employ a Newtonian method with exact line searches, where we were able to show finite convergence of both the inner and the outer algorithms. In our numerical experiments we compared our approach with a pure interior point approach, where we showed that although the interior point method was overall faster, our method was superior in solving a problem from near-optimal but non-interior initial points. This situation is relevant in at least three situations: sensitivity analysis, branch and bound methods and in sequential quadratic (or linear) programming methods for solving general nonlinear problems. This is due to the fact that our method does not need strict interiority or proximity to the central path in order to behave properly. It is also the case that the augmented Lagrangian approach was faster in circa $30\%$ of the problems, which is somewhat interesting.

The natural question that arises is whether our approach could be improved to be competitive with professional interior point codes. It is well known that sofisticated techniques for solving the linear systems arising in interior point iterations are available \cite{gondzio25}, either for the augmented system or for the normal equations, exploring sparsity and parallelisation. All these techniques can also be employed for enhancing our augmented Lagrangian method, and we leave to a future study a more robust comparison in this sense. Also, it remains to be investigated how to fine tune the regularization parameter to allow our method to solve linear programming problems more consistently.

In the context of augmented Lagrangian methods for general nonlinear programming, our results are somewhat surprising in the sense that performing a second-order Lagrange multiplier update, at least near the solution, was much better than the usual first-order update, which is used in most well known implementation. Besides results about superlinear convergence under the second-order update \cite{yaxyuan,convbertsekas}, this update has not been much explored in the literature. Of course, its main drawback is the necessity of inverting the Hessian of the augmented Lagrangian function, but, at least when the inverse is readily available, incorporating the second-order update should outperform methods using only the first-order update. It is also interesting to investigate alternative updates that do not need computation of the inverse but that use information about the Hessian, say, following a quasi-Newton approach.

\printbibliography
% \bibliographystyle{plain}
 %\bibliography{biblio}

\end{document}
